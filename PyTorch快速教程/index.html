
<!doctype html>
<html lang="zh" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
      <link rel="shortcut icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.1.2, mkdocs-material-7.0.3">
    
    
      
        <title>PyTorch快速教程 - 个人笔记</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.1655a90d.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.7fa14f5b.min.css">
        
          
          
          <meta name="theme-color" content="#009485">
        
      
    
    
    
      
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700%7CRoboto+Mono&display=fallback">
        <style>:root{--md-text-font-family:"Roboto";--md-code-font-family:"Roboto Mono"}</style>
      
    
    
    
    
      
    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="" data-md-color-primary="teal" data-md-color-accent="pink">
      
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#_1" class="md-skip">
          跳转至
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="个人笔记" class="md-header__button md-logo" aria-label="个人笔记">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            个人笔记
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              PyTorch快速教程
            
          </span>
        </div>
      </div>
    </div>
    <div class="md-header__options">
      
    </div>
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
      </label>
      
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="搜索" placeholder="搜索" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" data-md-state="active" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
      </label>
      <button type="reset" class="md-search__icon md-icon" aria-label="Clear" tabindex="-1">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41L17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41z"/></svg>
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            正在初始化搜索引擎
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    




<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="个人笔记" class="md-nav__button md-logo" aria-label="个人笔记">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54z"/></svg>

    </a>
    个人笔记
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_1" type="checkbox" id="__nav_1" >
      
      <label class="md-nav__link" for="__nav_1">
        一、计算机视觉专栏
        <span class="md-nav__icon md-icon"></span>
      </label>
      <nav class="md-nav" aria-label="一、计算机视觉专栏" data-md-level="1">
        <label class="md-nav__title" for="__nav_1">
          <span class="md-nav__icon md-icon"></span>
          一、计算机视觉专栏
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
  
  
  
    <li class="md-nav__item">
      <a href="../%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/" class="md-nav__link">
        目标检测论文解读
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../OCR%E6%96%B9%E5%90%91%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/" class="md-nav__link">
        OCR方向论文解读
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../%E4%BA%BA%E8%84%B8%E6%96%B9%E5%90%91%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/" class="md-nav__link">
        人脸方向论文解读
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../%E5%9B%BE%E5%83%8F%E8%AF%86%E5%88%AB%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/" class="md-nav__link">
        图像识别论文解读
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/" class="md-nav__link">
        深度学习基础
      </a>
    </li>
  

          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_2" type="checkbox" id="__nav_2" checked>
      
      <label class="md-nav__link" for="__nav_2">
        二、AI代码专栏
        <span class="md-nav__icon md-icon"></span>
      </label>
      <nav class="md-nav" aria-label="二、AI代码专栏" data-md-level="1">
        <label class="md-nav__title" for="__nav_2">
          <span class="md-nav__icon md-icon"></span>
          二、AI代码专栏
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          PyTorch快速教程
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        PyTorch快速教程
      </a>
      
        
<nav class="md-nav md-nav--secondary" aria-label="目录">
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      目录
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#_1" class="md-nav__link">
    基础用法
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pytorch" class="md-nav__link">
    Pytorch数据加载与处理
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#sample" class="md-nav__link">
    采样器Sample
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_2" class="md-nav__link">
    模型多卡训练
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pytorch_1" class="md-nav__link">
    Pytorch神经网络模块
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pytorch_2" class="md-nav__link">
    Pytorch搭建网络
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pytorch_3" class="md-nav__link">
    Pytorch初始化的两种方式
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pytorchhook" class="md-nav__link">
    Pytorch的钩子函数hook
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pytorchfunction" class="md-nav__link">
    Pytorch自定义新层(Function)
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pytorch_4" class="md-nav__link">
    Pytorch查看模型结构
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pytorchlossoptimizerlr_scheduler" class="md-nav__link">
    Pytorch的loss、optimizer、梯度裁剪、lr_scheduler
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pytorch_5" class="md-nav__link">
    Pytorch模型存储
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pytorch_6" class="md-nav__link">
    Pytorch迁移学习
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pytorch16" class="md-nav__link">
    Pytorch1.6支持混合精度训练
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pytorch_7" class="md-nav__link">
    PyTorch多线程训练
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_3" class="md-nav__link">
    问题
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../PaddlePaddle%E5%BF%AB%E9%80%9F%E6%95%99%E7%A8%8B/" class="md-nav__link">
        PaddlePaddle快速教程
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../caffe%E7%AE%80%E6%98%8E%E6%95%99%E7%A8%8B/" class="md-nav__link">
        Caffe快速教程
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../onnx%E7%AE%80%E6%98%8E%E6%95%99%E7%A8%8B/" class="md-nav__link">
        ONNX简明教程
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B7%A5%E5%85%B7%E4%BB%A3%E7%A0%81/" class="md-nav__link">
        深度学习工具代码
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../pandas%E3%80%81matplotlib%E7%AE%80%E6%B4%81%E7%AC%94%E8%AE%B0/" class="md-nav__link">
        PD+PLT简洁笔记
      </a>
    </li>
  

          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3" type="checkbox" id="__nav_3" >
      
      <label class="md-nav__link" for="__nav_3">
        三、常用工具专栏
        <span class="md-nav__icon md-icon"></span>
      </label>
      <nav class="md-nav" aria-label="三、常用工具专栏" data-md-level="1">
        <label class="md-nav__title" for="__nav_3">
          <span class="md-nav__icon md-icon"></span>
          三、常用工具专栏
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
  
  
  
    <li class="md-nav__item">
      <a href="../%E9%87%8F%E5%8C%96%E5%B7%A5%E5%85%B7%E4%BD%BF%E7%94%A8/" class="md-nav__link">
        量化工具使用
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../%E5%AE%9E%E7%94%A8%E5%B7%A5%E5%85%B7%E6%95%99%E7%A8%8B/" class="md-nav__link">
        实用工具教程
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../%E7%BD%91%E7%AB%99%E6%94%B6%E9%9B%86/" class="md-nav__link">
        学习网站收集
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA%E5%BA%93%28albumentations%2BAugmentor%29/" class="md-nav__link">
        图像增强库
      </a>
    </li>
  

          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_4" type="checkbox" id="__nav_4" >
      
      <label class="md-nav__link" for="__nav_4">
        四、编程语言专栏
        <span class="md-nav__icon md-icon"></span>
      </label>
      <nav class="md-nav" aria-label="四、编程语言专栏" data-md-level="1">
        <label class="md-nav__title" for="__nav_4">
          <span class="md-nav__icon md-icon"></span>
          四、编程语言专栏
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
  
  
  
    <li class="md-nav__item">
      <a href="../c%2B%2B%E7%AE%80%E6%98%8E%E6%95%99%E7%A8%8B/" class="md-nav__link">
        c++简明教程
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../vim_cmake_git/" class="md-nav__link">
        vim_git_cmake
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../python%E5%88%B7%E9%A2%98/" class="md-nav__link">
        python刷题
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../java%E7%AE%80%E6%98%8E%E6%95%99%E7%A8%8Band%E5%AE%89%E5%8D%93%E5%BC%80%E5%8F%91/" class="md-nav__link">
        java简明教程and安卓开发
      </a>
    </li>
  

          
        </ul>
      </nav>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary" aria-label="目录">
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      目录
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#_1" class="md-nav__link">
    基础用法
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pytorch" class="md-nav__link">
    Pytorch数据加载与处理
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#sample" class="md-nav__link">
    采样器Sample
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_2" class="md-nav__link">
    模型多卡训练
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pytorch_1" class="md-nav__link">
    Pytorch神经网络模块
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pytorch_2" class="md-nav__link">
    Pytorch搭建网络
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pytorch_3" class="md-nav__link">
    Pytorch初始化的两种方式
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pytorchhook" class="md-nav__link">
    Pytorch的钩子函数hook
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pytorchfunction" class="md-nav__link">
    Pytorch自定义新层(Function)
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pytorch_4" class="md-nav__link">
    Pytorch查看模型结构
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pytorchlossoptimizerlr_scheduler" class="md-nav__link">
    Pytorch的loss、optimizer、梯度裁剪、lr_scheduler
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pytorch_5" class="md-nav__link">
    Pytorch模型存储
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pytorch_6" class="md-nav__link">
    Pytorch迁移学习
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pytorch16" class="md-nav__link">
    Pytorch1.6支持混合精度训练
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pytorch_7" class="md-nav__link">
    PyTorch多线程训练
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_3" class="md-nav__link">
    问题
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content" data-md-component="content">
            <article class="md-content__inner md-typeset">
              
                
                
                  <h1>PyTorch快速教程</h1>
                
                <p><a href="https://mp.weixin.qq.com/s/CO_xZ7SqOl57iLAcupoBDA">Pytorch加速训练合集</a>|<a href="https://pytorch.apachecn.org/">Pytorch apachecn中文教程</a>|<a href="https://download.pytorch.org/whl/torch_stable.html">whl下载</a></p>
<h3 id="_1">基础用法<a class="headerlink" href="#_1" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="c1"># 固定随机种子</span>
<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">manual_seed_all</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># 指定GPU显卡</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;CUDA_VISIBLE_DEVICES&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;0,1&#39;</span>


<span class="c1"># 数据类型</span>
<span class="c1"># torch.FloatTensor() 32位浮点型 默认 # torch.cuda.FloatTensor()</span>
<span class="c1"># torch.DoubleTensor() 64位浮点型</span>
<span class="c1"># torch.ShortTensor() 16位整型</span>
<span class="c1"># torch.IntTensor() 32位整型</span>
<span class="c1"># torch.LongTensor() 64位整型</span>
<span class="c1"># 补充1，转换Tensor的数据类型 比如 a是IntTensor类型，只需要 a.float() 就可以转为FloatTensor类型</span>
<span class="n">tensor</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
<span class="n">tensor</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">long</span><span class="p">()</span>
<span class="c1"># 设置全局默认tensor数据类型</span>
<span class="n">torch</span><span class="o">.</span><span class="n">set_default_tensor_type</span><span class="p">(</span><span class="s1">&#39;torch.DoubleTensor&#39;</span><span class="p">)</span>


<span class="c1"># 创建tensor</span>
<span class="n">t</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="o">*</span><span class="n">sizes</span><span class="p">)</span> <span class="c1"># 创建tensor时，系统不会马上分配空间，只是会计算剩余的内存是否足够使用，使用到tensor时才会分配</span>
<span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">arr</span><span class="p">)</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">arr</span><span class="p">)</span> <span class="c1"># 基础构造函数</span>
<span class="c1"># 而下面的其它操作都是在创建完tensor之后马上进行空间分配。例如:torch.ones(2,3)</span>
<span class="c1"># 下面的这些创建方法都可以在创建的时候指定数据类型dtype和存放device(cpu/gpu).例如:t.randn(2, 3,device=t.device(&#39;cpu&#39;))</span>
<span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span> <span class="c1"># 全1Tensor 参数:*size</span>
<span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span> <span class="c1"># 全零的Tensor 参数:*size</span>
<span class="n">torch</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="o">*</span><span class="n">size</span><span class="p">)</span><span class="c1"># 对角线为1，其他为0</span>
<span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">s</span><span class="p">,</span><span class="n">e</span><span class="p">,</span><span class="n">step</span><span class="p">)</span><span class="c1"># 从s到e,范围[s,e)，步长为step</span>
<span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">s</span><span class="p">,</span><span class="n">e</span><span class="p">,</span><span class="n">steps</span><span class="p">)</span><span class="c1"># 从s到e,范围[s,e],均匀切分成steps份</span>
<span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="o">*</span><span class="n">size</span><span class="p">)</span><span class="o">/</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="o">*</span><span class="n">size</span><span class="p">)</span><span class="c1">#均匀/标准分布</span>
<span class="n">torch</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">mean</span><span class="p">,</span><span class="n">std</span><span class="p">)</span> <span class="c1"># 正态分布</span>
<span class="n">torch</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">from</span><span class="p">,</span><span class="n">to</span><span class="p">)</span> <span class="c1"># 均匀分布</span>
<span class="n">torch</span><span class="o">.</span><span class="n">randperm</span><span class="p">(</span><span class="n">n</span><span class="p">)</span> <span class="c1"># 随机排列,integers:[0,n-1],这些整数随机排列</span>
<span class="c1"># torch.*_like(tensora) 可以生成和tensora拥有同样属性(类型，形状，cpu/gpu)的新tensor。</span>
<span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">a</span><span class="p">)</span> <span class="c1">#等价于t.zeros(a.shape,dtype=a.dtype,device=a.device)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">t</span><span class="o">.</span><span class="n">int16</span><span class="p">)</span> <span class="c1">#可以修改某些属性</span>
<span class="n">torch</span><span class="o">.</span><span class="n">rand_like</span><span class="p">(</span><span class="n">a</span><span class="p">)</span> <span class="c1"># 和a属性一样，就是值不同</span>
<span class="c1"># tensor.new_*(new_shape) 新建一个不同形状的tensor，属性啥的相同</span>
<span class="n">a</span><span class="o">.</span><span class="n">new_ones</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">t</span><span class="o">.</span><span class="n">int</span><span class="p">)</span>
<span class="n">a</span><span class="o">.</span><span class="n">new_tensor</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">])</span>


<span class="c1"># 常用属性</span>
<span class="n">tensor_a</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">tensor_a</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="c1"># 也是返回torch.Size对象 等价于tensor_a.size()</span>
<span class="n">tensor_a</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="c1"># 可以获得Tensor的数值，而不是Tensor，而tensor_a[0]得到的还是tensor,只不过是0-dim，当然tensor_a[0].item()也可以，现在已经不是用[0]来获取值了</span>
<span class="n">tensor_a</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span> <span class="c1"># 把tensor转为list</span>
<span class="n">tensor_a</span><span class="o">.</span><span class="n">type</span><span class="p">()</span> <span class="c1"># Tensor的数据类型</span>
<span class="n">tensor_a</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="c1"># Tensor的维度信息</span>
<span class="n">tensor_a</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="c1"># Tensor中元素个数总数 等价于tensor_a.nelement()</span>


<span class="c1"># 常用方法</span>
<span class="c1"># 矩阵计算</span>
<span class="n">torch</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">tensor1</span><span class="p">,</span> <span class="n">tensor2</span><span class="p">)</span> <span class="c1"># (m*n) * (n*p) -&gt; (m*p).</span>
<span class="n">torch</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">tensor1</span><span class="p">,</span> <span class="n">tensor2</span><span class="p">)</span> <span class="c1"># (b*m*n) * (b*n*p) -&gt; (b*m*p).</span>
<span class="n">torch</span><span class="o">.</span><span class="n">addmm</span><span class="o">/</span><span class="n">addbmm</span><span class="o">/</span><span class="n">addmv</span><span class="o">/</span><span class="n">addr</span><span class="o">/</span><span class="n">baddbmm</span>
<span class="n">tensor1</span> <span class="o">*</span> <span class="n">tensor2</span> <span class="c1"># Element-wise multiplication.</span>
<span class="n">torch</span><span class="o">.</span><span class="n">dot</span><span class="o">/</span><span class="n">cross</span>    <span class="c1">#内积/外积</span>
<span class="n">torch</span><span class="o">.</span><span class="n">inverse</span>    <span class="c1">#求逆矩阵</span>
<span class="n">torch</span><span class="o">.</span><span class="n">svd</span>    <span class="c1">#奇异值分解</span>
<span class="n">tensor_a</span><span class="o">.</span><span class="n">t</span>    <span class="c1"># 转置</span>
<span class="c1"># 需要注意的是，矩阵的转置会导致存储空间不连续，需调用它的.contiguous方法将其转为连续</span>
<span class="n">tensor_a</span><span class="o">.</span><span class="n">t</span><span class="p">()</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span> <span class="c1"># 判断: tensor_a.t().is_contiguous()返回False，转过之后连续</span>
<span class="c1"># 逐元素操作</span>
<span class="n">torch</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">)</span> <span class="c1"># x+y 也可以 当然 x.add_(y) # 不返回值，直接对x修改,下面的都可以加_,例:x.squeeze_(0)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1"># 求均值</span>
<span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">)</span> <span class="c1"># 矩阵乘法,也可以用 torch.mm(x,y)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="nb">min</span><span class="p">,</span> <span class="nb">max</span><span class="p">)</span><span class="c1"># 截断:小于min的设置为min，大于max的设置为max</span>
<span class="c1"># torch.abs/sqrt/div/exp/fmod/log/pow..    绝对值/平方根/除法/指数/求余/求幂..</span>
<span class="c1"># torch.cos/sin/asin/atan2/cosh..    相关三角函数</span>
<span class="c1"># torch.ceil/round/floor/trunc    上取整/四舍五入/下取整/只保留整数部分</span>
<span class="c1"># torch.sigmod/tanh..    激活函数</span>
<span class="c1"># 总结经验:下面的函数大多都有两个属性: dim=0/1/..表示维度 keepdim=True会保留维度1 例如:输入(m,n,k)-&gt;dim=0-&gt;输出(1, n, k)或者(n, k)</span>
<span class="c1"># mean/sum/median/mode    均值/和/中位数/众数</span>
<span class="c1"># norm/dist    范数/距离</span>
<span class="c1"># std/var    标准差/方差</span>
<span class="c1"># cumsum/cumprod    累加/累乘(特殊)</span>
<span class="n">tensor_a</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># 沿着行进行累加，注意是使各行之间没联系，各加各的</span>
<span class="n">values</span><span class="p">,</span><span class="n">indexs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">topk</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">10</span><span class="p">),</span><span class="n">k</span><span class="p">)</span> <span class="c1"># 计算前topk个元素，返回值+索引</span>
<span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">tensor_a</span><span class="p">,[</span><span class="n">dim</span><span class="p">])</span><span class="o">/</span><span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">tensor_a</span><span class="p">,</span><span class="n">tensor_b</span><span class="p">)</span><span class="c1"># 大小</span>
<span class="n">max_value</span><span class="p">,</span><span class="n">max_idx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">tensor_a</span><span class="p">,</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> 
<span class="c1">#判断大小</span>
<span class="n">tensor_a</span><span class="o">&gt;</span><span class="mi">1</span> <span class="c1"># 返回一个ByteTensor,大于1的值为1，小于1的值为0</span>
<span class="n">tensor_a</span><span class="p">[</span><span class="n">tensor_a</span><span class="o">&gt;</span><span class="mi">1</span><span class="p">]</span> <span class="c1"># 返回tensor_a中大于1的tensor值 等价于tensor_a.masked_select(a&gt;1)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">index_select</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">index</span><span class="p">)</span><span class="c1"># 在指定维度dim上选取，比如选取某些行、某些列</span>
<span class="n">torch</span><span class="o">.</span><span class="n">masked_select</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span><span class="c1"># 例子如上，a[a&gt;0]，使用ByteTensor进行选取</span>
<span class="n">torch</span><span class="o">.</span><span class="n">non_zero</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span><span class="c1"># 非0元素的下标</span>
<span class="n">torch</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">index</span><span class="p">)</span><span class="c1"># 根据index，在dim维度上选取数据，输出的size与index一样</span>
<span class="c1"># gather是一个比较复杂的操作，对一个2维tensor，输出的每个元素如下：</span>
<span class="c1"># out[i][j] = input[index[i][j]][j]  # dim=0</span>
<span class="c1"># out[i][j] = input[i][index[i][j]]  # dim=1</span>
<span class="c1"># 举例子：torch.arange(0,16).view(4,4).gather(0,torch.LongTensor([[0,1,2,3]])) # 取正对角线元素</span>
<span class="c1"># gather相对应的逆操作是scatter_，gather把数据从input中按index取出，而scatter_是把取出的数据再放回去。注意scatter_函数是inplace操作。</span>
<span class="n">out</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">index</span><span class="p">)</span>
<span class="c1">#--&gt;近似逆操作</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">()</span>
<span class="n">out</span><span class="o">.</span><span class="n">scatter_</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">index</span><span class="p">)</span>


<span class="c1"># 操作维度</span>
<span class="c1"># 重点，增加维度unsqueeze,减少维度squeeze,维度重排 permute，维度交换 transpose, Tensor的reshape操作==tensor_a.view  0维是y轴，1维是x轴</span>
<span class="n">None类似于np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">,</span> <span class="n">为a新增了一个轴</span><span class="c1"># tensor_a.shape:(3,4) -&gt; tensor_a[None].shape:(1,3,4) 因为等价于 a[None,:,:]</span>
<span class="n">tensor_a</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="c1"># 在第一维度增加 参数是dim:0开始 (4,3) -&gt; (1,4,3) 等价于 tensor_a[None,:] ,负数维度表示倒数的维度</span>
<span class="n">tensor_a</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="c1"># 减少第一维度 参数是dim:0开始 (1,4,3) -&gt; (4,3)</span>
<span class="n">tensor_a</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span> <span class="c1"># 没有参数 将 tensor 中所有的一维全部都去掉 (1,1,4,3) -&gt; (4,3)</span>
<span class="n">tensor_a</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="o">*</span><span class="n">size</span><span class="p">)</span> <span class="c1"># 扩大张量(重复),不会占用额外空间，只会在需要的时候才扩充，可极大节省内存</span>
<span class="n">torch</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">7</span><span class="p">)</span> <span class="c1"># # 张量扩展 Expand tensor of shape 64*512 to shape 64*512*7*7.</span>
<span class="n">tensor_a</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span> <span class="c1"># 维度重新排列 (3,4,5) -&gt; (4,3,5)</span>
<span class="n">tensor_a</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span> <span class="c1"># 维度交换 (3,4,5) -&gt; (5,4,3)</span>
<span class="n">tensor_a</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">5</span><span class="p">)</span> <span class="c1"># Tensor reshape (3,4,5) -&gt; (12,5)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">shape</span><span class="p">)</span> <span class="c1"># torch.reshape 可以自动处理输入张量不连续的情况</span>
<span class="c1"># 如果新大小超过了原大小，会自动分配新的内存空间，而如果新大小小于原大小，则之前的数据依旧会被保存,但是只显示size大小的数据</span>
<span class="n">tensor_a</span><span class="o">.</span><span class="n">resize_</span><span class="p">()</span> <span class="c1"># 与view不同，它可以修改tensor的大小</span>

<span class="c1"># 张量拼接，torch.cat会沿着指定维度拼接，torch.stack会增加一维</span>
<span class="c1"># 例如当参数是 3 个 10×5 的张量，torch.cat 的结果是 30×5 的张量，而 torch.stack 的结果是 3×10×5 的张量</span>
<span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">seq</span><span class="p">,</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="n">out</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span><span class="c1"># 在指定维度上拼接张量,维度不会增加，只会在某一维度进行拼接</span>
<span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">((</span><span class="n">Tensor</span><span class="p">),</span><span class="n">dim</span><span class="p">)</span><span class="c1"># 会增加一个维度 a:(3,3),b:(3,3),torch.stack([a,b],0)-&gt;shape:(2,3,3),toch.stack([a,b],1)-&gt;shape:(3,2,3)</span>


<span class="c1"># tensor切片</span>
<span class="n">a</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">1</span><span class="p">,:</span><span class="mi">2</span><span class="p">]</span> <span class="c1"># 第0行，前两列  tensor([[-0.1855, -0.4570]])</span>
<span class="n">a</span><span class="p">[</span><span class="mi">0</span><span class="p">,:</span><span class="mi">2</span><span class="p">]</span> <span class="c1"># 注意两者的区别：形状不同 tensor([-0.1855, -0.4570])</span>
<span class="c1"># 高级索引 前提:x.shape-&gt;(3,3,3)</span>
<span class="n">x</span><span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">]]</span> <span class="c1"># x[1,1,2]和x[2,2,0]</span>
<span class="n">x</span><span class="p">[[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">]]</span> <span class="c1"># x[2,0,1],x[1,0,1],x[0,0,1]</span>
<span class="n">x</span><span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="o">...</span><span class="p">]</span> <span class="c1"># x[0] 和 x[2]</span>


<span class="c1"># 张量复制</span>
<span class="n">tensor</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span> <span class="c1"># memory:new,still in graph:yes</span>
<span class="n">tensor</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="c1"># memory:shared,still in graph:no</span>
<span class="n">tensor</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span><span class="c1"># memory:new,still in graph:no</span>

<span class="c1"># numpy &amp; tensor</span>
<span class="n">tensor_var</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span> <span class="c1"># CPU:tensor-&gt;numpy,GPU时:tensor_var.cpu().numpy()</span>
<span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">tensor_var</span><span class="p">)</span> <span class="c1"># numpy-&gt;tensor</span>


<span class="c1"># 求导，三变量</span>
<span class="c1"># x.data -&gt; Variable 变为Tensor</span>
<span class="c1"># x.grad -&gt; Variable的梯度</span>
<span class="c1"># x.grad_fn -&gt; Variable的梯度函数</span>
<span class="n">tensor_a</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">parameters</span><span class="p">)</span> <span class="c1"># 反向传播函数:接受的参数parameters必须要和tensor_a的大小一模一样,然后作为tensor_a的系数传回去</span>
<span class="c1"># 举例子</span>
<span class="n">tensor_a</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">tensor_a</span><span class="o">.</span><span class="n">size</span><span class="p">()))</span>
<span class="c1"># 注意：grad在反向传播过程中是累加的(accumulated)，这意味着每一次运行反向传播，梯度都会累加之前的梯度，所以反向传播之前需把梯度清零。</span>
<span class="n">tensor_a</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span> <span class="c1"># 以下划线结束的函数是inplace操作，会修改自身的值，就像add_</span>


<span class="c1"># pytorch使用GPU model = Net()</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="o">/</span><span class="s2">&quot;cuda:x&quot;</span><span class="p">)</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda:0&quot;</span> <span class="k">if</span> <span class="n">t</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span><span class="p">)</span> 
<span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span> <span class="c1"># 等价于 net = net.to(device)</span>
<span class="n">images</span> <span class="o">=</span> <span class="n">images</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">labels</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</code></pre></div>
<h3 id="pytorch">Pytorch数据加载与处理<a class="headerlink" href="#pytorch" title="Permanent link">&para;</a></h3>
<p><a href="https://blog.csdn.net/weixin_38533896/article/details/86028509">transforms的22个方法</a></p>
<p><strong>重写数据加载:<code>torch.utils.data.Dataset</code>,必须继承这个类，并且重写两个方法</strong></p>
<ul>
<li><code>__getitem__</code>：返回一条数据，或一个样本。<code>obj[index]</code>等价于<code>obj.__getitem__(index)</code></li>
<li><code>__len__</code>：返回样本的数量。len(obj)等价于<code>obj.__len__()</code></li>
</ul>
<div class="highlight"><pre><span></span><code><span class="c1"># 方法一:继承data.Dataset</span>
<span class="k">class</span> <span class="nc">DogCat</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="p">):</span> <span class="c1"># 必须继承data.Dataset类</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">root</span><span class="p">,</span><span class="n">transforms</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span><span class="c1"># root是指根目录,加入预处理</span>
        <span class="n">imgs</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="n">root</span><span class="p">)</span>
        <span class="c1">#指定路径，方便在__getitem__方法中读取图片 用列表是方便索引</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">imgs</span> <span class="o">=</span> <span class="p">[</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">root</span><span class="p">,</span><span class="n">img</span><span class="p">)</span> <span class="k">for</span> <span class="n">img</span> <span class="ow">in</span> <span class="n">imgs</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">transforms</span> <span class="o">=</span> <span class="n">transforms</span>

    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">index</span><span class="p">):</span>
        <span class="n">img_path</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">imgs</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>
        <span class="c1"># dog-&gt;1,cat-&gt;0</span>
        <span class="n">label</span> <span class="o">=</span> <span class="mi">1</span> <span class="k">if</span> <span class="s1">&#39;dog&#39;</span> <span class="ow">in</span> <span class="n">img_path</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;/&#39;</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;.&#39;</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span> <span class="k">else</span> <span class="mi">0</span>
        <span class="n">data</span> <span class="o">=</span> <span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">img_path</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">transforms</span><span class="p">:</span> <span class="c1"># 执行预处理操作</span>
            <span class="n">data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">transforms</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">array</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
            <span class="n">data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">array</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">data</span><span class="p">,</span><span class="n">label</span>

    <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">imgs</span><span class="p">)</span>


<span class="c1"># 图像增强</span>
<span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">transforms</span> <span class="k">as</span> <span class="n">T</span>
<span class="n">transform</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span>
    <span class="n">T</span><span class="o">.</span><span class="n">Resize</span><span class="p">(</span><span class="mi">224</span><span class="p">),</span> <span class="c1"># 缩放图片(Image)，保持长宽比不变，最短边为224像素</span>
    <span class="n">T</span><span class="o">.</span><span class="n">CenterCrop</span><span class="p">(</span><span class="mi">224</span><span class="p">),</span> <span class="c1"># 从图片中间切出224*224的图片</span>
    <span class="n">T</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span><span class="c1"># 将图片(Image)转成Tensor，归一化至[0, 1]</span>
    <span class="n">T</span><span class="o">.</span><span class="n">Normalize</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="p">[</span><span class="o">.</span><span class="mi">5</span><span class="p">,</span><span class="o">.</span><span class="mi">5</span><span class="p">,</span><span class="o">.</span><span class="mi">5</span><span class="p">],</span><span class="n">std</span><span class="o">=</span><span class="p">[</span><span class="o">.</span><span class="mi">5</span><span class="p">,</span><span class="o">.</span><span class="mi">5</span><span class="p">,</span><span class="o">.</span><span class="mi">5</span><span class="p">])</span>  <span class="c1"># 标准化至[-1, 1]，规定均值和标准差</span>
<span class="p">])</span>      

<span class="n">dataset</span> <span class="o">=</span> <span class="n">DogCat</span><span class="p">(</span><span class="s2">&quot;./dogs-vs-cats/train/&quot;</span><span class="p">,</span><span class="n">transforms</span><span class="o">=</span><span class="n">transform</span><span class="p">)</span>
<span class="n">img</span><span class="p">,</span><span class="n">label</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">img</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span><span class="n">label</span><span class="p">)</span><span class="c1"># torch.Size([3, 224, 224]) 1</span>
</code></pre></div>
<p><strong>ImageFolder</strong></p>
<div class="highlight"><pre><span></span><code><span class="c1"># 读取分类的目录结构</span>
<span class="n">DogCat</span>
  <span class="o">|-</span><span class="n">cat</span><span class="o">/</span> <span class="c1"># 因为ImageFolder定义的类名是从0开始的，一般建议直接把目录名改为从0开始</span>
    <span class="o">|-</span><span class="n">cat0</span><span class="o">.</span><span class="n">jpg</span>
    <span class="o">...</span>
  <span class="o">|-</span><span class="n">dog</span>
    <span class="o">|-</span><span class="n">dog0</span><span class="o">.</span><span class="n">jpg</span>
    <span class="o">...</span>


<span class="c1"># 一个方法:</span>
<span class="n">torchvision</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">ImageFolder</span><span class="p">(</span>
    <span class="n">root</span><span class="p">,</span><span class="c1"># 在root指定的路径下寻找图片</span>
    <span class="n">transform</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="c1"># 图像增强</span>
    <span class="n">target_transform</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="c1"># 对label转换</span>
    <span class="n">loader</span><span class="o">=&lt;</span><span class="n">function</span> <span class="n">default_loader</span> <span class="n">at</span> <span class="mh">0x11ed2b560</span><span class="o">&gt;</span><span class="p">,</span> <span class="c1"># 给定路径后如何读取图片，默认读取为RGB格式的PIL Image对象</span>
    <span class="n">is_valid_file</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span class="p">)</span>


<span class="c1"># 三个变量</span>
<span class="bp">self</span><span class="o">.</span><span class="n">classes</span> <span class="c1"># 用一个list保存 类名</span>
<span class="bp">self</span><span class="o">.</span><span class="n">class_to_idx</span> <span class="c1"># {类名:类序号(从0开始)}-&gt;{&#39;cat&#39;: 0, &#39;dog&#39;: 1}</span>
<span class="bp">self</span><span class="o">.</span><span class="n">imgs</span> <span class="c1"># [(imgpath1,0),(imgpath2,0),(imgpath3,1)...]</span>


<span class="c1"># ImageFolder的返回值，dataset</span>
<span class="n">dataset</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="c1"># 第一维度是第几张图，返回一个包含(图片对象，label)的对象,如果没有transform，返回的还是PIL</span>
<span class="n">dataset</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="c1"># 图片对象</span>
<span class="n">dataset</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span> <span class="c1"># label</span>


<span class="c1"># 一个小例子</span>
<span class="n">normalize</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="p">[</span><span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">],</span> <span class="n">std</span><span class="o">=</span><span class="p">[</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">])</span>
<span class="n">transform</span>  <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span>
         <span class="n">transforms</span><span class="o">.</span><span class="n">RandomResizedCrop</span><span class="p">(</span><span class="mi">224</span><span class="p">),</span>
         <span class="n">transforms</span><span class="o">.</span><span class="n">RandomHorizontalFlip</span><span class="p">(),</span>
         <span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
         <span class="n">normalize</span><span class="p">,</span>
<span class="p">])</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">ImageFolder</span><span class="p">(</span><span class="s1">&#39;DogCat/&#39;</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">)</span>
<span class="n">dataset</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span> <span class="c1"># CxHxW</span>
<span class="c1"># 看看图片</span>
<span class="n">to_img</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">ToPILImage</span><span class="p">()</span>
<span class="c1"># 0.2和0.4是标准差和均值的近似</span>
<span class="n">to_img</span><span class="p">(</span><span class="n">dataset</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="mf">0.2</span><span class="o">+</span><span class="mf">0.4</span><span class="p">)</span> <span class="c1"># 会显示图片</span>
</code></pre></div>
<p><strong>Dataset</strong></p>
<div class="highlight"><pre><span></span><code><span class="c1"># 上面的两种方式，一次只会返回一个(img,label),训练的时候一般是batch，所以</span>
<span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
    <span class="n">dataset</span><span class="p">,</span> <span class="c1"># 上面方式的返回值</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">sampler</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="c1"># 样本抽样</span>
    <span class="n">batch_sampler</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">num_workers</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="c1"># 多进程</span>
    <span class="n">collate_fn</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="c1"># 将若干图片拼接为一个batch的数据拼接方式，可以自定义</span>
    <span class="n">pin_memory</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span><span class="c1"># 是否将数据保存在pin memory区，pin memory中的数据转到GPU会快一些</span>
    <span class="n">drop_last</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">timeout</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">worker_init_fn</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">multiprocessing_context</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span class="p">)</span>


<span class="c1"># 小例子</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span> <span class="c1"># 导入包，注意位置</span>
<span class="n">dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">drop_last</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="c1"># 注意:dataloader返回的是个可迭代的对象，每个迭代对象包含batch_size个样本</span>
<span class="n">dataiter</span> <span class="o">=</span> <span class="nb">iter</span><span class="p">(</span><span class="n">dataloader</span><span class="p">)</span>
<span class="n">imgs</span><span class="p">,</span><span class="n">labels</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">dataiter</span><span class="p">)</span> <span class="c1"># batch_size个</span>
<span class="n">imgs</span><span class="o">.</span><span class="n">shape</span> <span class="c1"># torch.Size([32, 3, 224, 224])</span>
</code></pre></div>
<p><strong>补充:collate_fn函数</strong></p>
<div class="highlight"><pre><span></span><code><span class="c1"># 自定义这个函数可以实现任何你想要的输出</span>
<span class="k">def</span> <span class="nf">custom_collate</span><span class="p">(</span><span class="n">batch</span><span class="p">):</span> 
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    图像识别:这个batch是一个list，长度是batch_size，里面的元素是self.__getitem__(index)得到的元素:[(img1,label1),(img2,label2)]</span>
<span class="sd">    目标检测:这个batch可能是:[{&#39;image&#39;: img, &#39;bboxes&#39;: bbox, &#39;category_id&#39;: labels},....]</span>
<span class="sd">    总结:这个list里面是什么内容，取决于 self.__getitem__(index) return的是什么元素</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="c1"># 下面以图像识别返回的[img,label]进行改写</span>
    <span class="n">batch</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="c1"># 先按label长度进行排序</span>
    <span class="n">img</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">batch</span><span class="p">)</span>
    <span class="n">pad_label</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">lens</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">max_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">label</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="c1"># 进行长度的pad</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">label</span><span class="p">)):</span>
        <span class="n">temp_label</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">max_len</span>
        <span class="n">temp_label</span><span class="p">[:</span><span class="nb">len</span><span class="p">(</span><span class="n">label</span><span class="p">[</span><span class="n">i</span><span class="p">])]</span> <span class="o">=</span> <span class="n">label</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="n">pad_label</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">temp_label</span><span class="p">)</span>
        <span class="n">lens</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">label</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>
    <span class="k">return</span> <span class="n">img</span><span class="p">,</span> <span class="n">pad_label</span><span class="p">,</span> <span class="n">lens</span> <span class="c1"># [img,label,label_len]</span>
</code></pre></div>
<p><img alt="image-20200301123715359" src="../assets/image-20200301123715359.png" /></p>
<h3 id="sample">采样器Sample<a class="headerlink" href="#sample" title="Permanent link">&para;</a></h3>
<p><strong>采样Sample介绍:<code>torch.utils.data.sampler.xx</code></strong>，注意，<strong>每个取样器返回的都是样本在dataset中的索引，并不是样本本身</strong></p>
<div class="highlight"><pre><span></span><code><span class="c1"># 1.SequentialSampler(data_source) # 顺序采样，只有一个参数dataset。返回以一个与数据集等长的迭代器</span>

<span class="c1"># 2.RandomSampler(data_source,replacement=False,num_samples=None) # 随机采样</span>
<span class="c1"># 2.1 data_source (Dataset) – dataset to sample from</span>
<span class="c1"># 2.2 replacement (bool)  放回/不放回采样，默认不放回</span>
<span class="c1"># 2.3 num_samples  (python:int) – default=`len(dataset)`.replacement=True时使用；默认是选择所有数据，当放回采样时可以设定随机选取多少个数据。</span>

<span class="c1"># 3.WeightedRandomSampler(weights,num_samples,replacement=True)</span>
<span class="c1"># 3.1 weights (sequence)赋予每个样本权重。代表取到该样例的概率。数据不均衡时可以用来控制不同类别样本的采样权重</span>
<span class="c1"># 3.2 num_samples (python:int) – number of samples to draw</span>
<span class="c1"># 3.2 replacement (bool) – if True, samples are drawn with replacement. If not, they are drawn without replacement, which means that when a sample index is drawn for a row, it cannot be drawn again for that row.</span>

<span class="c1"># final 配合DataLoader一起使用</span>
<span class="n">sample</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">xx</span> <span class="c1"># 加这一步是为了取样，例如数据是20，可以先取样10，如果batch_size=2，那么dataloader的len是5</span>
<span class="n">daloloader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">data</span><span class="p">,</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span><span class="n">sampler</span><span class="o">=</span><span class="n">sample</span><span class="p">)</span>
</code></pre></div>
<h3 id="_2">模型多卡训练<a class="headerlink" href="#_2" title="Permanent link">&para;</a></h3>
<p><a href="https://zhuanlan.zhihu.com/p/158375055">博客-重要</a>|<a href="https://github.com/tczhangzhi/pytorch-distributed/blob/master/distributed.py">代码</a></p>
<h3 id="pytorch_1">Pytorch神经网络模块<a class="headerlink" href="#pytorch_1" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="c1"># 简单概述一下</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="c1"># 进入神经网络，不得不提 nn.Parameter 和Variable类似，但是默认是求梯度的</span>
<span class="n">w</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span> <span class="c1"># 这是nn.Parameter变量w</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span> <span class="c1"># 这是nn.Parameter变量 b</span>
<span class="c1"># 神经网络中常用的参数</span>
<span class="n">F</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1"># 求x的sigmoid函数</span>
<span class="n">F</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1"># tanh函数</span>
<span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<span class="c1"># 卷积层,输入的shape=(N,C,H,W) 输出相同shape</span>
<span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span>
    <span class="n">in_channels</span><span class="p">,</span> <span class="c1"># int</span>
    <span class="n">out_channels</span><span class="p">,</span> <span class="c1"># int</span>
    <span class="n">kernel_size</span><span class="p">,</span><span class="c1"># int or tuple</span>
    <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="c1"># int or tuple</span>
    <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="c1"># int or tuple,输入的每一条边补充0的层数</span>
    <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="c1"># int or tuple，空洞卷积，卷积核元素之间的间距</span>
    <span class="n">groups</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="c1"># int 从输入通道到输出通道的阻塞连接数</span>
    <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="c1"># bool</span>
    <span class="n">padding_mode</span><span class="o">=</span><span class="s1">&#39;zeros&#39;</span><span class="p">,</span>
<span class="p">)</span>


<span class="c1"># BN层,输入的shape=(N,C,H,W) 输出相同shape</span>
<span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span>
    <span class="n">num_features</span><span class="p">,</span> <span class="c1"># 输入特征图(N,C,H,W)中的C</span>
    <span class="n">eps</span><span class="o">=</span><span class="mf">1e-05</span><span class="p">,</span><span class="c1"># 为保证数值稳定性（分母不能趋近或取0）,给分母加上的值。默认为1e-5</span>
    <span class="n">momentum</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span><span class="c1"># 动态均值和动态方差所使用的动量。默认为0.1</span>
    <span class="n">affine</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="c1">#是否需要仿射:如果False,那么gamma=1,beta=0,且不会被学习</span>
    <span class="n">track_running_stats</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="c1"># # track_running_stats=True表示跟踪整个训练过程中的batch的统计特性，得到方差和均值，而不只是仅仅依赖与当前输入的batch的统计特性。相反的，如果track_running_stats=False那么就只是计算当前输入的batch的统计特性中的均值和方差了。当在推理阶段的时候，如果track_running_stats=False，此时如果batch_size比较小，那么其统计特性就会和全局统计特性有着较大偏差，可能导致糟糕的效果。</span>
<span class="p">)</span>


<span class="c1"># 池化层,输入的shape=(N,C,H,W) 输出相同shape</span>
<span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span>
    <span class="n">kernel_size</span><span class="p">,</span> <span class="c1"># max pooling 的窗口大小</span>
    <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="c1"># 移动步长，默认是kernel_size</span>
    <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="c1"># 输入的每一条边填充0的层数</span>
    <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="c1"># 控制窗口中元素步幅</span>
    <span class="n">return_indices</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="c1"># True,会返回输出最大值的序号，对于上采样操作会有帮助</span>
    <span class="n">ceil_mode</span><span class="o">=</span><span class="kc">False</span> <span class="c1"># 如果等于True，计算输出信号大小的时候，会使用向上取整，代替默认的向下取整的操作</span>
    <span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">AvgPool2d</span><span class="p">(</span>
    <span class="n">kernel_size</span><span class="p">,</span> <span class="c1"># 池化窗口大小</span>
    <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="c1"># 移动步长</span>
    <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="c1"># 输入的每一条边补充0的层数</span>
    <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="c1"># 一个控制窗口中元素步幅的参数</span>
    <span class="n">ceil_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> 
    <span class="n">count_include_pad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="c1"># 全局平均池化层</span>
<span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveMaxPool2d</span><span class="p">(</span>
    <span class="n">output_size</span><span class="p">,</span> <span class="c1"># 输出尺寸，可以用（H,W）表示H*W的输出，也可以使用数字H表示H*H大小的输出</span>
    <span class="n">return_indices</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span><span class="c1"># 如果设置为True，会返回输出的索引</span>


<span class="c1"># 图片上采样</span>
<span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">interpolate</span><span class="p">(</span>
    <span class="nb">input</span><span class="p">,</span> <span class="c1"># 输入</span>
    <span class="n">size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">scale_factor</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="c1"># 放大的倍数</span>
    <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;nearest&#39;</span><span class="p">,</span> <span class="c1"># &quot;nearest&quot;/&quot;area&quot;/&quot;linear&quot;</span>
    <span class="n">align_corners</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span class="p">)</span>


<span class="c1"># 转置卷积</span>
<span class="c1"># 对于每一条边输入输出的尺寸的公式如下:output = (input-1)*stride+outputpadding-2*padding+kernelsize</span>
<span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ConvTranspose2d</span><span class="p">(</span>
    <span class="n">in_channels</span><span class="p">,</span> <span class="c1"># int</span>
    <span class="n">out_channels</span><span class="p">,</span> <span class="c1"># int</span>
    <span class="n">kernel_size</span><span class="p">,</span> <span class="c1"># int or tuple</span>
    <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="c1"># int or tuple,将要输入扩大的倍数</span>
    <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="c1"># int or tuple，输入的每一条边补充0的层数，高宽都增加2*padding</span>
    <span class="n">output_padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="c1"># int or tuple，输出边补充0的层数，高宽都增加padding</span>
    <span class="n">groups</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="c1"># int 从输入通道到输出通道的阻塞连接数</span>
    <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="c1"># bool</span>
    <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="c1"># int or tuple,卷积核元素之间的间距</span>
    <span class="n">padding_mode</span><span class="o">=</span><span class="s1">&#39;zeros&#39;</span><span class="p">,</span>
<span class="p">)</span>


<span class="c1"># 全连接层</span>
<span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span>
    <span class="n">in_features</span><span class="p">,</span> 
    <span class="n">out_features</span><span class="p">,</span> 
    <span class="n">bias</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>


<span class="c1"># Dropout,以一定的概率闭合神经元</span>
<span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span>
    <span class="n">p</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
    <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span>
<span class="p">)</span>


<span class="c1"># 激活函数</span>
<span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()</span>
<span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Tanh</span><span class="p">()</span>
<span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Softmax</span><span class="p">()</span>
<span class="c1"># ReLU激活:inplace如果设为True，它会把输出直接覆盖到输入中，这样可以节省内存/显存</span>
<span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="p">)</span> <span class="c1"># inplace默认为False,ReLU函数有个inplace参数，如果设为True,它会把输出直接覆盖到输入中，这样可以节省内反向传播的梯度。但是只有少数的autograd</span>
<span class="c1"># f(x) = max(0,x)+negative_slope*min(0,x) negative_slope：控制负斜率的角度</span>
<span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">LeakyReLU</span><span class="p">(</span><span class="n">negative_slope</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span><span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> 
<span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU6</span><span class="p">(</span><span class="n">inplace</span><span class="p">)</span> <span class="c1">#ReLU6(x) = min(max(0,x), 6)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Threshold</span><span class="p">(</span><span class="n">threshold</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="c1">#阈值。输入值小于阈值则会被value代替</span>
</code></pre></div>
<h3 id="pytorch_2">Pytorch搭建网络<a class="headerlink" href="#pytorch_2" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="c1"># 搭建网络前必须先介绍一下pytorch的初始化</span>
<span class="kn">from</span> <span class="nn">torch.nn</span> <span class="kn">import</span> <span class="n">init</span>
<span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_uniform</span><span class="p">(</span><span class="n">seq_net</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span> <span class="c1"># Xavier 初始化方法</span>


<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">from</span> <span class="nn">torch.nn</span> <span class="kn">import</span> <span class="n">functional</span> <span class="k">as</span> <span class="n">F</span>
<span class="c1"># 方法一</span>
<span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span><span class="c1"># 继承nn.Module</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span> <span class="c1"># 里面必须是个类</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">16</span> <span class="o">*</span> <span class="mi">5</span> <span class="o">*</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">120</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">120</span><span class="p">,</span> <span class="mi">84</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">84</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
        <span class="c1"># 修改初始化</span>
        <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">modules</span><span class="p">():</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">):</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">kaiming_normal_</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">detach</span><span class="p">())</span>
                <span class="n">m</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">):</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">kaiming_normal_</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">detach</span><span class="p">())</span>
                <span class="n">m</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span> <span class="c1"># 前项传播函数，backward函数就会自动调用</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">pool</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)),</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">pool</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">)),</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">16</span> <span class="o">*</span> <span class="mi">5</span> <span class="o">*</span> <span class="mi">5</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

<span class="c1"># 方式二</span>
<span class="n">seq_net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="c1"># 里面必须是个类</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">),</span> 
    <span class="n">nn</span><span class="o">.</span><span class="n">Tanh</span><span class="p">(),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="p">)</span>
<span class="c1"># 模型层的一些常用参数</span>
<span class="n">seq_net</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="c1"># 第一层，通过索引访问每一层，用其他两种有名字，也可以根据名字取</span>
<span class="n">seq_net</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span> <span class="c1"># 第一层的权重的module</span>
<span class="n">seq_net</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span> <span class="c1"># 第一层的权重</span>
<span class="n">seq_net</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">bias</span> <span class="c1"># 第一层的bias的参数</span>


<span class="c1"># 方式三</span>
<span class="n">net1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">()</span>
<span class="n">net1</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span><span class="s1">&#39;conv&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="n">net1</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span><span class="s1">&#39;batchnorm&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="mi">3</span><span class="p">))</span>
<span class="n">net1</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span><span class="s1">&#39;activation_layer&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">())</span>


<span class="c1"># 方式四</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">OrderedDict</span>
<span class="n">net3</span><span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">OrderedDict</span><span class="p">([</span>
          <span class="p">(</span><span class="s1">&#39;conv1&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)),</span>
          <span class="p">(</span><span class="s1">&#39;bn1&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="mi">3</span><span class="p">)),</span>
          <span class="p">(</span><span class="s1">&#39;relu1&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">())</span>
        <span class="p">]))</span>

<span class="c1"># 1.遍历每一层,net._modules.items()</span>
<span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="k">for</span> <span class="n">name</span><span class="p">,</span><span class="n">layer</span> <span class="ow">in</span> <span class="n">net</span><span class="o">.</span><span class="n">_modules</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">name</span><span class="p">,</span><span class="n">layer</span><span class="p">)</span> <span class="c1"># conv1  Conv2d(3,6,kernel_size=(5,5),stride=(1,1))</span>
<span class="n">net</span><span class="o">.</span><span class="n">conv1</span> <span class="c1"># 直接通过名字访问某层</span>
<span class="n">net</span><span class="o">.</span><span class="n">conv1</span><span class="o">.</span><span class="n">weight</span> <span class="c1">#打印该层权重module，torch.nn.parameter.Parameter对象</span>
<span class="n">net</span><span class="o">.</span><span class="n">conv1</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span> <span class="c1"># 获取该层的权重值</span>
<span class="n">net</span><span class="o">.</span><span class="n">conv1</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">grad</span> <span class="c1"># 梯度</span>
<span class="n">net</span><span class="o">.</span><span class="n">conv1</span><span class="o">.</span><span class="n">bias</span><span class="c1"># 打印该层偏置</span>

<span class="c1"># 3.附加提取信息(可以看看)</span>
<span class="c1"># 3.1 net.modules() &amp; net.named_modules() 返回的是所有的元素，包括不同级别的子元素，model-&gt;第一层由浅入深逐层遍历-&gt;第二层由浅入深逐层遍历-&gt;...-&gt;最后一层的外层-&gt;...-&gt;最后一层的最里面一层。</span>
<span class="c1"># 常用作下面的结构</span>
<span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">modules</span><span class="p">():</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">)</span> <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">):</span>
        <span class="k">pass</span>
<span class="c1"># 3.2 net.children() &amp; net.named_children() 返回的是最外层的元素</span>

<span class="c1"># 3.3.模型的一些常用参数</span>
<span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">()</span> <span class="c1"># 获得模型的参数，这样在 torch.optim.SGD(param,0.1) 就可以用了,循环遍历的话每个参数其实就是 net.conv1.weight</span>
<span class="c1"># for name,parameters in net.named_parameters(): return 名字+参数</span>
<span class="n">model</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">()</span> <span class="c1"># 可同时返回可学习的参数及名称。例: conv1.weight:参数</span>
<span class="n">model</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span> <span class="c1"># 所有参数的梯度清零</span>

<span class="c1"># 4.访问每一层的各个属性，包括batchnorm的running_mean/bn.running_var</span>
<span class="c1"># 返回的是一个字典，一般是存到.pth模型中的数据</span>
<span class="k">for</span> <span class="n">k</span><span class="p">,</span><span class="n">v</span> <span class="ow">in</span> <span class="n">net</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">k</span><span class="p">,</span><span class="n">v</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="c1"># net.named_parames() 只访问weight/bias，batchnorm的runing_xx不能</span>

<span class="c1"># 5.ModuleList:可以像用list一样使用它，但不能直接把输入传给ModuleList</span>
<span class="c1"># 为什么不直接用List呢，干嘛多此一举呢这是因为ModuleList是Module的子类，当在Module中使用它的时候，就能自动识别为子module。</span>
<span class="n">modellist</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">),</span> 
    <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span> 
    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
<span class="p">])</span>
<span class="c1"># output = modelist(input) 会报错,因为modellist没有实现forward方法</span>
<span class="c1"># 可以这样使用</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="k">for</span> <span class="n">model</span> <span class="ow">in</span> <span class="n">modellist</span><span class="p">:</span>
    <span class="nb">input</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>

<span class="c1"># 6.ParameterList,类似ModuleList</span>
<span class="bp">self</span><span class="o">.</span><span class="n">params</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ParameterList</span><span class="p">([</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">)])</span>
</code></pre></div>
<h3 id="pytorch_3">Pytorch初始化的两种方式<a class="headerlink" href="#pytorch_3" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">torch.nn</span> <span class="kn">import</span> <span class="n">init</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">from</span> <span class="nn">torch.nn</span> <span class="kn">import</span> <span class="n">functional</span> <span class="k">as</span> <span class="n">F</span>

<span class="c1"># 方式一，model.apply(func_name)：apply函数可以不断遍历model的各个模块。实际上其使用的是深度优先算法</span>
<span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span><span class="c1"># 继承nn.Module</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span> <span class="c1"># 里面必须是个类</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">16</span> <span class="o">*</span> <span class="mi">5</span> <span class="o">*</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">120</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">120</span><span class="p">,</span> <span class="mi">84</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">84</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span> <span class="c1"># 前项传播函数，backward函数就会自动调用</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">pool</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)),</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">pool</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">)),</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">16</span> <span class="o">*</span> <span class="mi">5</span> <span class="o">*</span> <span class="mi">5</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
<span class="c1"># 1. 根据网络层的不同定义不同的初始化方式     </span>
<span class="k">def</span> <span class="nf">weight_init</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">):</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_normal_</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">constant_</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="c1"># 也可以判断是否为conv2d，使用相应的初始化方式 </span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">):</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">kaiming_normal_</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;fan_out&#39;</span><span class="p">,</span> <span class="n">nonlinearity</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">)</span>
     <span class="c1"># 是否为批归一化层</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">):</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">constant_</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">constant_</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="c1"># 2. 初始化网络结构        </span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Net</span><span class="p">(</span><span class="n">in_dim</span><span class="p">,</span> <span class="n">n_hidden_1</span><span class="p">,</span> <span class="n">n_hidden_2</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">)</span>
<span class="c1"># 3. 将weight_init应用在子模块上</span>
<span class="n">model</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">weight_init</span><span class="p">)</span>

<span class="c1"># 方式二，利用self.modules()来进行循环</span>
<span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span><span class="c1"># 继承nn.Module</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span> <span class="c1"># 里面必须是个类</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">16</span> <span class="o">*</span> <span class="mi">5</span> <span class="o">*</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">120</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">120</span><span class="p">,</span> <span class="mi">84</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">84</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
        <span class="c1"># 修改初始化</span>
        <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">modules</span><span class="p">():</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">):</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">kaiming_normal_</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">detach</span><span class="p">())</span>
                <span class="n">m</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">):</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">kaiming_normal_</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">detach</span><span class="p">())</span>
                <span class="n">m</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span> <span class="c1"># 前项传播函数，backward函数就会自动调用</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">pool</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)),</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">pool</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">)),</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">16</span> <span class="o">*</span> <span class="mi">5</span> <span class="o">*</span> <span class="mi">5</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</code></pre></div>
<h3 id="pytorchhook">Pytorch的钩子函数hook<a class="headerlink" href="#pytorchhook" title="Permanent link">&para;</a></h3>
<blockquote>
<p>原文博客链接:<a href="https://zhuanlan.zhihu.com/p/75054200">link1</a>|<a href="https://cloud.tencent.com/developer/article/1122582">link2</a></p>
<ol>
<li>
<p><a href="https://link.zhihu.com/?target=http%3A//pytorch.org/docs/autograd.html%3Fhighlight%3Dhook%23torch.autograd.Variable.register_hook">torch.autograd.Variable.register_hook</a> (torch.tensor.register_hook）</p>
</li>
<li>
<p><a href="https://link.zhihu.com/?target=http%3A//pytorch.org/docs/nn.html%3Fhighlight%3Dhook%23torch.nn.Module.register_backward_hook">torch.nn.Module.register_backward_hook</a> (Python method, in torch.nn)</p>
</li>
<li>
<p><a href="https://link.zhihu.com/?target=http%3A//pytorch.org/docs/nn.html%3Fhighlight%3Dhook%23torch.nn.Module.register_forward_hook">torch.nn.Module.register_forward_hook</a> </p>
</li>
</ol>
<p>第一个是register_hook，是针对Variable对象的，后面的两个：register_backward_hook和register_forward_hook是针对nn.Module这个对象的。</p>
</blockquote>
<p>利用它，我们可以**不必改变网络输入输出的结构，方便地获取、改变网络中间层变量的值和梯度**</p>
<p><strong>pytorch痛点:中间变量的释放</strong></p>
<p><img alt="image-20200928110857773" src="../assets/image-20200928110857773.png" /></p>
<ul>
<li>在 PyTorch 的计算图（computation graph）中，<strong>只有叶子结点（leaf nodes）的变量会保留梯度</strong>。而所有中间变量的梯度只被用于反向传播，一旦完成反向传播，<strong>中间变量的梯度就将自动释放</strong>，从而节约内存。</li>
</ul>
<div class="highlight"><pre><span></span><code><span class="c1"># 1.自动求导机制中只保存叶子节点，下面计算中，也就是中间变量在计算完成梯度后会自动释放以节省空间，所以下面代码我们在计算过程中只得到了z对x的梯度。</span>
<span class="n">In</span><span class="p">[</span><span class="mi">2</span><span class="p">]:</span> <span class="kn">import</span> <span class="nn">torch</span>
<span class="n">In</span><span class="p">[</span><span class="mi">3</span><span class="p">]:</span> <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">],</span><span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span><span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="c1"># 只要有一个tersor设为True,那么接下来的计算中所有相关的tensor都会支持自动求导求梯度。</span>
<span class="n">In</span><span class="p">[</span><span class="mi">4</span><span class="p">]:</span> <span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="mi">2</span> <span class="c1"># 中间变量y</span>
<span class="n">In</span><span class="p">[</span><span class="mi">5</span><span class="p">]:</span> <span class="n">z</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> 
<span class="n">In</span><span class="p">[</span><span class="mi">7</span><span class="p">]:</span> <span class="n">z</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="n">In</span><span class="p">[</span><span class="mi">8</span><span class="p">]:</span> <span class="n">x</span><span class="o">.</span><span class="n">grad</span> <span class="c1"># tensor([ 1.,  1.])</span>
<span class="n">In</span><span class="p">[</span><span class="mi">9</span><span class="p">]:</span> <span class="n">y</span><span class="o">.</span><span class="n">grad</span> <span class="c1"># None 应该为(0.5,0.5)</span>
<span class="n">In</span><span class="p">[</span><span class="mi">10</span><span class="p">]:</span> <span class="n">z</span><span class="o">.</span><span class="n">grad</span> <span class="c1"># None 应该为1</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="c1"># 2.如果想获得z对y的梯度，解决方案就是hook，钩子函数:register_hook的作用是，当反传时，除了完成原有的反传，额外多完成一些任务。你可以定义一个中间变量的hook，将它的grad值打印出来，当然你也可以定义一个全局列表，将每次的grad值添加到里面去。</span>
<span class="c1"># 2.1 register_hook(hook_fun):这个函数属于torch.tensor类,梯度计算时会自动执行，参数是一个函数</span>
<span class="c1"># hook_fun函数格式是：def hook_fun(grad): return Tensor/None,我们可以改变这个hook函数的返回值，但是不能改变其参数。</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="n">grad_list</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">def</span> <span class="nf">print_grad</span><span class="p">(</span><span class="n">grad</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;y_grad:&quot;</span><span class="p">,</span><span class="n">grad</span><span class="p">)</span>
    <span class="n">grad_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">grad</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">grad_list</span>
<span class="n">In</span><span class="p">[</span><span class="mi">3</span><span class="p">]:</span> <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">],</span><span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span><span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">In</span><span class="p">[</span><span class="mi">4</span><span class="p">]:</span> <span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="mi">2</span>
<span class="n">In</span><span class="p">[</span><span class="mi">5</span><span class="p">]:</span> <span class="n">y</span><span class="o">.</span><span class="n">requires_grad</span> <span class="c1"># True</span>
<span class="n">In</span><span class="p">[</span><span class="mi">6</span><span class="p">]:</span> <span class="n">y</span><span class="o">.</span><span class="n">register_hook</span><span class="p">(</span><span class="n">print_grad</span><span class="p">)</span>
<span class="n">In</span><span class="p">[</span><span class="mi">7</span><span class="p">]:</span> <span class="n">z</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="n">In</span><span class="p">[</span><span class="mi">8</span><span class="p">]:</span> <span class="n">z</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span> <span class="c1"># 打印出 y_grad:tensor([ 0.5000,  0.5000]),证明y的hook函数执行了</span>
</code></pre></div>
<p><strong>Hook for Modules</strong></p>
<p><code>register_forward_hook</code> 和 <code>register_backward_hook</code>，分别用来获取正/反向传播时，中间层模块输入和输出的 <code>feature/gradient</code>，大大降低了获取模型内部信息流的难度。</p>
<p><strong>register_forward_hook</strong></p>
<ul>
<li><code>register_forward_hook</code>的作用是**获取前向传播过程中，各个网络模块的输入和输出**，对于模块 <code>module</code>，其使用方式为：<code>module.register_forward_hook(hook_fn)</code>&rarr;<code>hook_fn(module, input, output)：1.2版本开始有返回值了，可以修改网络模块的输出</code></li>
</ul>
<div class="highlight"><pre><span></span><code><span class="c1"># xxx-&gt;model</span>
<span class="c1"># 全局变量，用于存储中间层的 feature</span>
<span class="n">total_feat_out</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">total_feat_in</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">def</span> <span class="nf">hook_fn_forward</span><span class="p">(</span><span class="n">m</span><span class="p">,</span><span class="nb">input</span><span class="p">,</span><span class="n">output</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;model module:&quot;</span><span class="p">,</span><span class="n">m</span><span class="p">)</span>
    <span class="n">total_feat_out</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
    <span class="n">total_feat_in</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>

<span class="k">for</span> <span class="n">name</span><span class="p">,</span><span class="n">module</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">named_children</span><span class="p">():</span>
    <span class="n">module</span><span class="o">.</span><span class="n">register_forward_hook</span><span class="p">(</span><span class="n">hook_fn_forward</span><span class="p">)</span> <span class="c1"># 这样会收集每个模块的输入和输出</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]])</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span>
<span class="n">o</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">o</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

<span class="c1"># 这样运行过后就能打印其输入和输出了，其实经过前向传播后就会输出结果了</span>
</code></pre></div>
<p><strong>register backward hook</strong></p>
<p><code>register_backward_hook</code> 的作用是获取神经网络反向传播过程中，各个模块**输入端和输出端的梯度值**。对于模块 module，其使用方式为：<code>module.register_backward_hook(hook_fn)--&gt;hook_fn(module, grad_input, grad_output) -&gt; Tensor or None</code></p>
<ul>
<li><code>hook_fn</code>：输入变量分别为：模块，模块输入端的梯度，模块输出端的梯度。需要注意的是，这里的**输入端**和**输出端**，是站在前向传播的角度的，而不是反向传播的角度。例如线性模块：<code>o=W*x+b</code>，其输入端为 W，x 和 b，输出端为 o。</li>
<li>如果模块有多个输入或者输出的话，<code>grad_input</code>和<code>grad_output</code>可以是 tuple 类型。对于线性模块：<code>o=W*x+b</code> ，它的输入端包括了W、x 和 b 三部分，因此 <code>grad_input</code> 就是一个包含三个元素的 tuple，例如:</li>
<li>在卷积层中，<code>bias</code> 的梯度位于<code>tuple</code> 的末尾：<code>grad_input</code> = (对<code>feature</code>的导数，对权重 <code>W</code>的导数，对 <code>bias</code> 的导数)</li>
<li>在全连接层中，<code>bias</code> 的梯度位于 <code>tuple</code> 的开头：<code>grad_input</code>=(对<code>bias</code>的导数，对<code>feature</code> 的导数，对 <code>W</code> 的导数)</li>
<li>在<code>forward hook</code> 中，<code>input</code> 是<code>x</code>，而不包括<code>W</code>和<code>b</code>。返回 <code>Tensor</code> 或者<code>None</code>，<code>backward hook</code> 函数不能直接改变它的输入变量，但是可以返回新的 <code>grad_input</code>，反向传播到它上一个模块。</li>
</ul>
<div class="highlight"><pre><span></span><code><span class="c1"># xxx-&gt;model</span>
<span class="c1"># 全局变量，用于存储中间层的 feature</span>
<span class="n">total_grad_out</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">total_grad_in</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">def</span> <span class="nf">hook_fn_backward</span><span class="p">(</span><span class="n">m</span><span class="p">,</span><span class="n">grad_input</span><span class="p">,</span><span class="n">grad_output</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;model module:&quot;</span><span class="p">,</span><span class="n">m</span><span class="p">)</span>
    <span class="n">total_grad_in</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">grad_input</span><span class="p">)</span>
    <span class="n">total_grad_out</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">grad_output</span><span class="p">)</span>

<span class="k">for</span> <span class="n">name</span><span class="p">,</span><span class="n">module</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">named_children</span><span class="p">():</span>
    <span class="n">module</span><span class="o">.</span><span class="n">register_backward_hook</span><span class="p">(</span><span class="n">hook_fn_backward</span><span class="p">)</span> <span class="c1"># 这样会收集每个模块的梯度输入和输出</span>

<span class="c1">#  这里的 requires_grad 很重要，如果不加，backward hook 执行到第一层，对 x 的导数将为 None</span>
<span class="c1"># 此外再强调一遍 x 的维度，一定不能写成 torch.Tensor([1.0, 1.0, 1.0]).requires_grad_()</span>
<span class="c1"># 否则 backward hook 会出问题。</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]])</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span>
<span class="n">o</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">o</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

<span class="c1"># 这样运行过后就能打印其输入和输出了，其实经过前向传播后就会输出结果了</span>
</code></pre></div>
<ul>
<li><code>register_backward_hook</code>只能操作简单模块，而不能操作包含多个子模块的复杂模块。 如果对复杂模块用了 backward hook，那么我们只能得到该模块最后一次简单操作的梯度信息。对于上面的代码稍作修改，不再遍历各个子模块，而是把 model 整体绑在一个 <code>hook_fn_backward</code>上：</li>
</ul>
<div class="highlight"><pre><span></span><code><span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">register_backward_hook</span><span class="p">(</span><span class="n">hook_fn_backward</span><span class="p">)</span> 

<span class="c1"># out：发现程序只输出了fc2的梯度</span>
<span class="n">Model</span><span class="p">(</span>
  <span class="p">(</span><span class="n">fc1</span><span class="p">):</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
  <span class="p">(</span><span class="n">relu1</span><span class="p">):</span> <span class="n">ReLU</span><span class="p">()</span>
  <span class="p">(</span><span class="n">fc2</span><span class="p">):</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="p">)</span>
<span class="n">grad_output</span> <span class="p">(</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">1.</span><span class="p">]]),)</span>
<span class="n">grad_input</span> <span class="p">(</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.</span><span class="p">]),</span> <span class="n">tensor</span><span class="p">([[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">,</span> <span class="mf">4.</span><span class="p">]]),</span> <span class="n">tensor</span><span class="p">([[</span> <span class="mf">7.</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">0.</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">27.</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">0.</span><span class="p">]]))</span>
</code></pre></div>
<h3 id="pytorchfunction">Pytorch自定义新层(Function)<a class="headerlink" href="#pytorchfunction" title="Permanent link">&para;</a></h3>
<ul>
<li>**方式一：**通过继承<code>torch.nn.Module</code>类来实现拓展。只需重新实现<code>__init__</code>和<code>forward</code>函数。</li>
<li>Module：只需定义<code>__init__</code>和<code>forward</code>，而<code>backward</code>的计算由自动求导机制构成</li>
<li><strong>方式二</strong>：通过继承<code>torch.autograd.Function</code>，除了要实现<code>__init__</code>和<code>forward</code>函数，还要实现<code>backward</code>函数;如果要自定义求导规则(例如二值化网络有些不可导函数)，就要用着个方法.</li>
<li><code>Function</code>需要定义三个方法：<code>__init__</code>,<code>forward</code>,<code>backward</code>（需要自己写求导公式）；</li>
</ul>
<div class="highlight"><pre><span></span><code><span class="c1"># 1.定义一个ReLU类别</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch.autograd</span> <span class="kn">import</span> <span class="n">Variable</span>

<span class="k">class</span> <span class="nc">MyReLU</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">Function</span><span class="p">):</span>
    <span class="c1"># 在forward中，需要定义MyReLU这个运算的forward计算过程</span>
    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">save_for_backward</span><span class="p">(</span><span class="n">input_</span><span class="p">)</span> <span class="c1"># 将输入保存起来，在backward时使用</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">input_</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="nb">min</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>   <span class="c1"># relu就是截断负数，让所有负数等于0</span>
        <span class="k">return</span> <span class="n">output</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">):</span>
        <span class="c1"># 根据BP算法的推导（链式法则），dloss / dx = (dloss / doutput) * (doutput / dx)</span>
        <span class="c1"># grad_output：dloss / doutput、</span>
        <span class="c1"># 因此只需求relu的导数，再乘以grad_outpu </span>
        <span class="n">input_</span><span class="p">,</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">saved_tensors</span>
        <span class="n">grad_input</span> <span class="o">=</span> <span class="n">grad_output</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
        <span class="n">grad_input</span><span class="p">[</span><span class="n">input_</span><span class="o">&lt;</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># 上诉计算的结果就是左式。即ReLU在反向传播中可以看做一个通道选择函数，所有未达到阈值（激活值&lt;0）的单元的梯度都为0,激活值&gt;0的乘以relu的梯度1</span>
        <span class="k">return</span> <span class="n">grad_input</span>

<span class="c1"># 2.验证Variable与Function的关系</span>
<span class="kn">from</span> <span class="nn">torch.autograd</span> <span class="kn">import</span> <span class="n">Variable</span>
<span class="n">input_</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
<span class="n">relu</span> <span class="o">=</span> <span class="n">MyReLU</span><span class="p">()</span>
<span class="n">output_</span> <span class="o">=</span> <span class="n">relu</span><span class="p">(</span><span class="n">input_</span><span class="p">)</span> <span class="c1"># output_.creator==relu,这个relu对象将output与input连接起来，形成计算图</span>

<span class="c1"># 封装成一个relu函数</span>
<span class="k">def</span> <span class="nf">relu</span><span class="p">(</span><span class="n">input_</span><span class="p">):</span>
    <span class="c1"># MyReLU()是创建一个MyReLU对象，</span>
    <span class="c1"># Function类利用了Python __call__操作，使得可以直接使用对象调用__call__制定的方法</span>
    <span class="c1"># __call__指定的方法是forward，因此下面这句MyReLU（）（input_）相当于</span>
    <span class="c1"># return MyReLU().forward(input_)</span>
    <span class="k">return</span> <span class="n">MyReLU</span><span class="p">()(</span><span class="n">input_</span><span class="p">)</span>
</code></pre></div>
<h3 id="pytorch_4">Pytorch查看模型结构<a class="headerlink" href="#pytorch_4" title="Permanent link">&para;</a></h3>
<p><a href="http://spytensor.com/index.php/archives/20/?fsbuba=v3lnw1">卷积可视化-GradCAM-keras/pytorch</a>|<a href="https://github.com/gautamMalu/caffe-gradCAM">caffe版本</a>:caffe默认没有损失就不算梯度<a href="https://www.jianshu.com/p/f93e462c7286">解决</a></p>
<div class="highlight"><pre><span></span><code><span class="c1"># github:https://github.com/sksq96/pytorch-summary</span>
<span class="kn">from</span> <span class="nn">torchsummary</span> <span class="kn">import</span> <span class="n">summary</span>
<span class="c1"># input_size 是根据你自己的网络模型的输入尺寸进行设置。</span>
<span class="n">summary</span><span class="p">(</span><span class="n">your_model</span><span class="p">,</span> <span class="n">input_size</span><span class="o">=</span><span class="p">(</span><span class="n">channels</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">W</span><span class="p">))</span>
</code></pre></div>
<h3 id="pytorchlossoptimizerlr_scheduler">Pytorch的loss、optimizer、梯度裁剪、lr_scheduler<a class="headerlink" href="#pytorchlossoptimizerlr_scheduler" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="c1"># 1.pytorch的loss函数 更多见下面</span>
<span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span> <span class="c1"># 线性回归的loss,均方误差</span>
<span class="n">nn</span><span class="o">.</span><span class="n">BCEWithLogitsLoss</span><span class="p">()</span> <span class="c1"># Logistic回归的二分类loss</span>
<span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span><span class="c1"># 用来计算交叉熵损失</span>
<span class="c1"># 用法 在训练上要两步</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span> <span class="c1"># 定义loss对象</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">y_hat</span><span class="p">,</span><span class="n">y_true</span><span class="p">)</span> <span class="c1"># 第一步，计算loss</span>
<span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span> <span class="c1"># 第二步</span>

<span class="c1"># pytorch的优化器 torch.optim.+Tab键</span>
<span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">([</span><span class="n">w</span><span class="p">,</span><span class="n">b</span><span class="p">],</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span><span class="c1"># 归 0 梯度，在自动求导前</span>
<span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span><span class="c1"># 然后使用这个函数来更新参数就可以了</span>

<span class="c1"># 2.常用优化器 更多见下面</span>
<span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">params</span><span class="p">,</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span> <span class="c1"># 随机梯度下降</span>
<span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">params</span><span class="p">,</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span><span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span> <span class="c1"># 动量法</span>
<span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adagrad</span><span class="p">(</span><span class="n">params</span><span class="p">,</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">RMSprop</span><span class="p">(</span><span class="n">params</span><span class="p">,</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adadelta</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">rho</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">params</span><span class="p">,</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="c1"># 用法举例(在训练时要两步)</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="nn">optim</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span> <span class="c1"># 新建一个优化器，指定要调整的参数和学习率</span>
<span class="c1"># 训练过程中 梯度清零(与net.zero_grad()效果一样)</span>
<span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span> <span class="c1"># 第一步</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span><span class="n">target</span><span class="p">)</span> <span class="c1"># 计算损失</span>
<span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span> <span class="c1"># 反向传播</span>
<span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span> <span class="c1"># 第二步 更新参数</span>

<span class="c1"># 优化器普通用法:一个参数组</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span><span class="n">weight_decay</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">)</span>
<span class="c1"># 优化器高级用法:不同子网络设置不同的学习率，如果对某个参数不指定学习率，就使用最外层的默认学习率</span>
<span class="n">optimizer</span> <span class="o">=</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">([</span> <span class="c1"># list里面套dict</span>
        <span class="p">{</span><span class="s1">&#39;params&#39;</span><span class="p">:</span> <span class="n">net</span><span class="o">.</span><span class="n">features</span><span class="o">.</span><span class="n">parameters</span><span class="p">()},</span> <span class="c1"># 学习率为1e-5</span>
        <span class="p">{</span><span class="s1">&#39;params&#39;</span><span class="p">:</span> <span class="n">net</span><span class="o">.</span><span class="n">classifier</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="mf">1e-2</span><span class="p">}</span>
    <span class="p">],</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">)</span>

<span class="c1"># 优化器的两个参数</span>
<span class="n">optimizer</span><span class="o">.</span><span class="n">defaults</span><span class="c1"># 字典，存放这个优化器的一些初始参数，有：&#39;lr&#39;, &#39;betas&#39;, &#39;eps&#39;, &#39;weight_decay&#39;, &#39;amsgrad&#39;。</span>
<span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span> <span class="c1"># 列表，每个元素都是一个字典，每个元素包含的关键字有：&#39;params&#39;, &#39;lr&#39;, &#39;betas&#39;, &#39;eps&#39;, &#39;weight_decay&#39;, &#39;amsgrad&#39;，params类是各个网络的参数放在了一起。</span>
<span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span> <span class="c1"># 获取学习率</span>
<span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">le</span><span class="o">-</span><span class="mi">5</span> <span class="c1"># 修改学习率</span>
<span class="c1"># 为了防止有多个参数组，我们可以使用一个循环</span>
<span class="k">for</span> <span class="n">param_group</span> <span class="ow">in</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">:</span>
    <span class="n">param_group</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1e-1</span>
<span class="c1"># 笨方法设置学习率衰减</span>
<span class="c1"># 设置学习衰减率</span>
<span class="k">def</span> <span class="nf">set_learning_rate</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">lr</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">param_group</span> <span class="ow">in</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">:</span>
        <span class="n">param_group</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">lr</span>
<span class="c1"># 之后在epoch的for循环里可以调用该方法</span>
<span class="c1"># 假设起初 lr = 0.1</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">30</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">epoch</span> <span class="o">==</span> <span class="mi">20</span><span class="p">:</span>
        <span class="n">set_learning_rate</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span><span class="mf">0.01</span><span class="p">)</span>


<span class="c1"># 3.梯度裁剪</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="n">loss</span><span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
<span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">clip_grad_norm_</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">max_norm</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">norm_type</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
<span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">clip_grad_norm_</span><span class="p">(</span>
    <span class="n">parameters</span><span class="p">,</span> <span class="c1"># 一个基于变量的迭代器，会进行梯度归一化</span>
    <span class="n">max_norm</span><span class="p">,</span> <span class="c1"># 梯度的最大范数</span>
    <span class="n">norm_type</span><span class="o">=</span><span class="mi">2</span> <span class="c1"># 规定范数的类型，默认为L2</span>
<span class="p">)</span>


<span class="c1"># 4.学习率衰减,以LambdaLR举例子:更新策略，new_lr = lr_lambda*init_lr</span>
<span class="n">scheduler</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">LambdaLR</span><span class="p">(</span>
    <span class="n">optimizer</span><span class="p">,</span> <span class="c1"># 在将optimizer传给scheduler后，在shcduler类的__init__方法中会给optimizer.param_groups列表中的那个元素（字典）增加一个key = &quot;initial_lr&quot;的元素表示初始学习率，等于optimizer.defaults[&#39;lr&#39;]。</span>
    <span class="n">lr_lambda</span><span class="p">,</span> <span class="c1"># 根据epoch计算λ的函数；或者是一个list的这样的function，分别计算各个parameter groups的学习率更新用到的λ；</span>
    <span class="n">last_epoch</span><span class="o">=-</span><span class="mi">1</span> <span class="c1"># 最后一个epoch的index，如果是训练了很多个epoch后中断了，继续训练，这个值就等于加载的模型的epoch。默认为-1表示从头开始训练，即从epoch=1开始。</span>
<span class="p">)</span>

<span class="c1"># 举例子</span>
<span class="n">initial_lr</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">net_1</span> <span class="o">=</span> <span class="n">model</span><span class="p">()</span>
<span class="n">optimizer_1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">net_1</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span> <span class="o">=</span> <span class="n">initial_lr</span><span class="p">)</span>
<span class="n">scheduler_1</span> <span class="o">=</span> <span class="n">LambdaLR</span><span class="p">(</span><span class="n">optimizer_1</span><span class="p">,</span> <span class="n">lr_lambda</span><span class="o">=</span><span class="k">lambda</span> <span class="n">epoch</span><span class="p">:</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;初始化的学习率：&quot;</span><span class="p">,</span> <span class="n">optimizer_1</span><span class="o">.</span><span class="n">defaults</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">])</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">11</span><span class="p">):</span>
    <span class="c1"># train</span>
    <span class="n">optimizer_1</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">optimizer_1</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;第</span><span class="si">%d</span><span class="s2">个epoch的学习率：</span><span class="si">%f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="n">optimizer_1</span><span class="o">.</span><span class="n">param_groups</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;lr&#39;</span><span class="p">]))</span>
    <span class="n">scheduler_1</span><span class="o">.</span><span class="n">step</span><span class="p">()</span> <span class="c1"># 学习率的调整应该放在optimizer更新之后,1.1.0之前的版本是放在之前的</span>

<span class="c1"># 其他学习率衰减函数详解:https://blog.csdn.net/qyhaill/article/details/103043637</span>
<span class="c1"># 每过step_size个epoch，做一次更新</span>
<span class="k">class</span> <span class="nc">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">StepLR</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">step_size</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">last_epoch</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="c1"># 每次遇到milestones中的epoch，做一次更新</span>
<span class="k">class</span> <span class="nc">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">MultiStepLR</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">milestones</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">last_epoch</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="c1"># 每个epoch都做一次更新：</span>
<span class="k">class</span> <span class="nc">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">ExponentialLR</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">last_epoch</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="c1"># 让lr随着epoch的变化图类似于cos</span>
<span class="k">class</span> <span class="nc">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">CosineAnnealingLR</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">T_max</span><span class="p">,</span> <span class="n">eta_min</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">last_epoch</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="c1"># 给定一个metric，当metric停止优化时减小学习率。</span>
<span class="k">class</span> <span class="nc">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">ReduceLROnPlateau</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;min&#39;</span><span class="p">,</span> <span class="n">factor</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">patience</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">threshold</span><span class="o">=</span><span class="mf">0.0001</span><span class="p">,</span> <span class="n">threshold_mode</span><span class="o">=</span><span class="s1">&#39;rel&#39;</span><span class="p">,</span> <span class="n">cooldown</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">min_lr</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-08</span><span class="p">)</span>
<span class="o">...</span> <span class="c1"># 还有一些，不介绍了</span>
</code></pre></div>
<h3 id="pytorch_5">Pytorch模型存储<a class="headerlink" href="#pytorch_5" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="n">model</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="c1"># 方式一(不推荐)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model</span><span class="p">,</span><span class="s2">&quot;model.pth&quot;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;model.pth&quot;</span><span class="p">)</span> <span class="c1"># cpu-&gt;cpu,gpu-&gt;gpu，注意这时tensor属于同块cpu/gpu</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;model.pth&quot;</span><span class="p">,</span><span class="n">map_location</span><span class="o">=</span><span class="k">lambda</span> <span class="n">storage</span><span class="p">,</span> <span class="n">loc</span><span class="p">:</span> <span class="n">storage</span><span class="p">)</span> <span class="c1"># gpu-&gt;cpu加载</span>
<span class="c1"># 或者 torch.load(&#39;model.pth&#39;, map_location=&#39;cpu&#39;)  # pytorch0.4.0及以上版本</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;model.pth&#39;</span><span class="p">,</span> <span class="n">map_location</span><span class="o">=</span><span class="k">lambda</span> <span class="n">storage</span><span class="p">,</span> <span class="n">loc</span><span class="p">:</span> <span class="n">storage</span><span class="o">.</span><span class="n">cuda</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span><span class="c1">#cpu-&gt;gpu</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;model.pth&#39;</span><span class="p">,</span> <span class="n">map_location</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;cuda:1&#39;</span><span class="p">:</span><span class="s1">&#39;cuda:0&#39;</span><span class="p">})</span> <span class="c1">#gpu1-&gt;gpu0</span>

<span class="c1"># 方式二(推荐)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span><span class="s2">&quot;model.pth&quot;</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;model.pth&#39;</span><span class="p">))</span>

<span class="c1"># 方式三(推荐)包含的信息有，epochID, state_dict, min loss,optimizer, 自定义损失函数的两个参数</span>
<span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">({</span><span class="s1">&#39;epoch&#39;</span><span class="p">:</span> <span class="n">epochID</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> 
            <span class="s1">&#39;state_dict&#39;</span><span class="p">:</span> <span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> 
            <span class="s1">&#39;best_loss&#39;</span><span class="p">:</span> <span class="n">lossMIN</span><span class="p">,</span>
            <span class="s1">&#39;optimizer&#39;</span><span class="p">:</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span>
            <span class="s1">&#39;alpha&#39;</span><span class="p">:</span> <span class="n">loss</span><span class="o">.</span><span class="n">alpha</span><span class="p">,</span> <span class="s1">&#39;gamma&#39;</span><span class="p">:</span> <span class="n">loss</span><span class="o">.</span><span class="n">gamma</span><span class="p">},</span>
           <span class="n">checkpoint_path</span> <span class="o">+</span> <span class="s1">&#39;/m-&#39;</span> <span class="o">+</span> <span class="n">launchTimestamp</span> <span class="o">+</span> <span class="s1">&#39;-&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">%.4f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">lossMIN</span><span class="p">)</span> <span class="o">+</span> <span class="s1">&#39;.pth.tar&#39;</span><span class="p">)</span>
<span class="c1"># 加载</span>
<span class="k">def</span> <span class="nf">load_checkpoint</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">checkpoint_PATH</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">checkpoint</span> <span class="o">!=</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">model_CKPT</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">checkpoint_PATH</span><span class="p">)</span>
        <span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">model_CKPT</span><span class="p">[</span><span class="s1">&#39;state_dict&#39;</span><span class="p">])</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;loading checkpoint!&#39;</span><span class="p">)</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">model_CKPT</span><span class="p">[</span><span class="s1">&#39;optimizer&#39;</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span>
</code></pre></div>
<h3 id="pytorch_6">Pytorch迁移学习<a class="headerlink" href="#pytorch_6" title="Permanent link">&para;</a></h3>
<p><code>model.train()指定当前模型model为训练状态,model.eval()指定当前模型为测试状态。</code></p>
<p><strong>train.py</strong></p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">os</span>
<span class="c1"># os.environ[&quot;CUDA_VISIBLE_DEVICES&quot;] = &quot;0&quot;  </span>
<span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">transforms</span>
<span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>
<span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">models</span>
<span class="kn">from</span> <span class="nn">torch.optim</span> <span class="kn">import</span> <span class="n">lr_scheduler</span>
<span class="kn">from</span> <span class="nn">torch.utils.tensorboard</span> <span class="kn">import</span> <span class="n">SummaryWriter</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">optim</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">copy</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">from</span> <span class="nn">tqdm</span> <span class="kn">import</span> <span class="n">tqdm</span>


<span class="c1"># cfg</span>
<span class="c1"># traindir = &quot;data/train&quot;</span>
<span class="c1"># validdir = &quot;data/valid&quot;</span>
<span class="n">traindir</span> <span class="o">=</span> <span class="s2">&quot;/home/cyy/4T/project/roi_delivery/libs/pig_up_down/data/train&quot;</span>
<span class="n">validdir</span> <span class="o">=</span> <span class="s2">&quot;/home/cyy/4T/project/roi_delivery/libs/pig_up_down/data/valid&quot;</span>

<span class="c1"># save weights</span>
<span class="n">save_weight</span> <span class="o">=</span> <span class="s2">&quot;weights&quot;</span>
<span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">save_weight</span><span class="p">,</span><span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># save log</span>
<span class="n">save_log</span> <span class="o">=</span> <span class="s2">&quot;logs&quot;</span>
<span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">save_log</span><span class="p">,</span><span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">tb_writer</span> <span class="o">=</span> <span class="n">SummaryWriter</span><span class="p">(</span><span class="n">log_dir</span><span class="o">=</span><span class="n">save_log</span><span class="p">)</span>

<span class="c1"># class num</span>
<span class="n">class_num</span> <span class="o">=</span> <span class="mi">3</span>

<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda:0&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span><span class="p">)</span>

<span class="c1"># Image transformations</span>
<span class="n">normal_val</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span><span class="mf">0.5</span><span class="p">,</span><span class="mf">0.5</span><span class="p">]</span>
<span class="n">image_transforms</span> <span class="o">=</span> <span class="p">{</span>
    <span class="c1"># Train uses data augmentation</span>
    <span class="s1">&#39;train&#39;</span><span class="p">:</span>
    <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span>
        <span class="n">transforms</span><span class="o">.</span><span class="n">Resize</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="mi">256</span><span class="p">),</span>
        <span class="n">transforms</span><span class="o">.</span><span class="n">RandomApply</span><span class="p">([</span><span class="n">transforms</span><span class="o">.</span><span class="n">RandomRotation</span><span class="p">(</span><span class="n">degrees</span><span class="o">=</span><span class="mi">30</span><span class="p">),</span>
                                <span class="n">transforms</span><span class="o">.</span><span class="n">ColorJitter</span><span class="p">(</span><span class="n">brightness</span><span class="o">=</span><span class="mf">0.1</span><span class="p">),</span>
                                <span class="n">transforms</span><span class="o">.</span><span class="n">RandomHorizontalFlip</span><span class="p">()],</span><span class="n">p</span><span class="o">=</span><span class="mf">0.5</span><span class="p">),</span>
        <span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
        <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">(</span><span class="n">normal_val</span><span class="p">,</span>
                             <span class="n">normal_val</span><span class="p">)</span>  <span class="c1"># Imagenet standards</span>
    <span class="p">]),</span>
    <span class="c1"># Validation does not use augmentation</span>
    <span class="s1">&#39;valid&#39;</span><span class="p">:</span>
    <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span>
        <span class="n">transforms</span><span class="o">.</span><span class="n">Resize</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="mi">256</span><span class="p">),</span>
        <span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
        <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">(</span><span class="n">normal_val</span><span class="p">,</span> <span class="n">normal_val</span><span class="p">)</span>
    <span class="p">]),</span>
<span class="p">}</span>

<span class="c1"># Datasets from folders</span>
<span class="n">data</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;train&#39;</span><span class="p">:</span>
    <span class="n">datasets</span><span class="o">.</span><span class="n">ImageFolder</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="n">traindir</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">image_transforms</span><span class="p">[</span><span class="s1">&#39;train&#39;</span><span class="p">]),</span>
    <span class="s1">&#39;valid&#39;</span><span class="p">:</span>
    <span class="n">datasets</span><span class="o">.</span><span class="n">ImageFolder</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="n">validdir</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">image_transforms</span><span class="p">[</span><span class="s1">&#39;valid&#39;</span><span class="p">]),</span>
<span class="p">}</span>

<span class="c1"># Dataloader iterators, make sure to shuffle</span>
<span class="n">dataloaders</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;train&#39;</span><span class="p">:</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;train&#39;</span><span class="p">],</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
    <span class="s1">&#39;valid&#39;</span><span class="p">:</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;valid&#39;</span><span class="p">],</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="p">}</span>

<span class="n">dataset_sizes</span> <span class="o">=</span> <span class="p">{</span><span class="n">x</span><span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">x</span><span class="p">])</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;train&#39;</span><span class="p">,</span> <span class="s1">&#39;valid&#39;</span><span class="p">]}</span>
<span class="n">class_names</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;train&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">classes</span>
<span class="nb">print</span><span class="p">(</span><span class="n">dataset_sizes</span><span class="p">,</span><span class="n">class_names</span><span class="p">)</span>

<span class="n">model_ft</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">resnet18</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="c1"># 将所有的参数层进行冻结</span>
<span class="c1"># for param in model_ft.parameters():</span>
    <span class="c1"># param.requires_grad = False</span>
<span class="n">num_ftrs</span> <span class="o">=</span> <span class="n">model_ft</span><span class="o">.</span><span class="n">fc</span><span class="o">.</span><span class="n">in_features</span>
<span class="n">model_ft</span><span class="o">.</span><span class="n">fc</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">num_ftrs</span><span class="p">,</span> <span class="n">class_num</span><span class="p">)</span>
<span class="n">model_ft</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>

<span class="c1"># Observe that all parameters are being optimized</span>
<span class="n">optimizer_ft</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model_ft</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>

<span class="c1"># Decay LR by a factor of 0.1 every 7 epochs</span>

<span class="n">exp_lr_scheduler</span> <span class="o">=</span> <span class="n">lr_scheduler</span><span class="o">.</span><span class="n">StepLR</span><span class="p">(</span><span class="n">optimizer_ft</span><span class="p">,</span> <span class="n">step_size</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>



<span class="c1"># train</span>
<span class="k">def</span> <span class="nf">train_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">scheduler</span><span class="p">,</span> <span class="n">num_epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">since</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

    <span class="n">best_model_wts</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">())</span>
    <span class="n">best_acc</span> <span class="o">=</span> <span class="mf">0.0</span>

    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Epoch </span><span class="si">{}</span><span class="s1">/</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="n">num_epochs</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;-&#39;</span> <span class="o">*</span> <span class="mi">10</span><span class="p">)</span>

        <span class="c1"># Each epoch has a training and validation phase</span>
        <span class="k">for</span> <span class="n">phase</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;train&#39;</span><span class="p">,</span> <span class="s1">&#39;valid&#39;</span><span class="p">]:</span>
            <span class="k">if</span> <span class="n">phase</span> <span class="o">==</span> <span class="s1">&#39;train&#39;</span><span class="p">:</span>
                <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>  <span class="c1"># Set model to training mode</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>  <span class="c1"># Set model to evaluate mode</span>

            <span class="n">running_loss</span> <span class="o">=</span> <span class="mf">0.0</span>
            <span class="n">running_corrects</span> <span class="o">=</span> <span class="mi">0</span>

            <span class="c1"># Iterate over data.</span>
            <span class="k">for</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">dataloaders</span><span class="p">[</span><span class="n">phase</span><span class="p">],</span><span class="n">desc</span><span class="o">=</span><span class="n">phase</span><span class="p">):</span>
                <span class="c1"># print(inputs.shape)</span>

                <span class="c1"># wrap them in Variable</span>
                <span class="n">inputs</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
                <span class="n">labels</span> <span class="o">=</span> <span class="n">labels</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

                <span class="c1"># zero the parameter gradients</span>
                <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

                <span class="c1"># forward</span>
                <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
                <span class="c1"># print(outputs.data)</span>
                <span class="c1"># print(&quot;*&quot;*30)</span>

                <span class="n">_</span><span class="p">,</span> <span class="n">preds</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>

                <span class="c1"># backward + optimize only if in training phase</span>
                <span class="k">if</span> <span class="n">phase</span> <span class="o">==</span> <span class="s1">&#39;train&#39;</span><span class="p">:</span>
                    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
                    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
                    <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

                <span class="c1"># statistics</span>
                <span class="n">running_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">*</span> <span class="n">inputs</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
                <span class="n">running_corrects</span> <span class="o">+=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">preds</span> <span class="o">==</span> <span class="n">labels</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
                <span class="c1"># print(f&#39;Loss:{loss.data.item()*inputs.size(0)}&#39;)</span>

            <span class="n">epoch_loss</span> <span class="o">=</span> <span class="n">running_loss</span> <span class="o">/</span> <span class="n">dataset_sizes</span><span class="p">[</span><span class="n">phase</span><span class="p">]</span>
            <span class="n">epoch_acc</span> <span class="o">=</span> <span class="n">running_corrects</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">/</span> <span class="n">dataset_sizes</span><span class="p">[</span><span class="n">phase</span><span class="p">]</span>


            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="si">{}</span><span class="s1"> Loss: </span><span class="si">{:.4f}</span><span class="s1"> Acc: </span><span class="si">{:.4f}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
            <span class="n">phase</span><span class="p">,</span> <span class="n">epoch_loss</span><span class="p">,</span> <span class="n">epoch_acc</span><span class="p">))</span>

            <span class="c1"># deep copy the model</span>
            <span class="k">if</span> <span class="n">phase</span> <span class="o">==</span> <span class="s1">&#39;valid&#39;</span> <span class="ow">and</span> <span class="n">epoch_acc</span> <span class="o">&gt;</span> <span class="n">best_acc</span><span class="p">:</span>
                <span class="n">best_acc</span> <span class="o">=</span> <span class="n">epoch_acc</span>
                <span class="n">best_model_wts</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">())</span>
            <span class="k">if</span> <span class="n">phase</span> <span class="o">==</span> <span class="s2">&quot;train&quot;</span><span class="p">:</span>
                <span class="n">tb_writer</span><span class="o">.</span><span class="n">add_scalar</span><span class="p">(</span><span class="s1">&#39;train/loss&#39;</span><span class="p">,</span><span class="n">epoch_loss</span><span class="p">,</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
                <span class="n">tb_writer</span><span class="o">.</span><span class="n">add_scalar</span><span class="p">(</span><span class="s1">&#39;train/acc&#39;</span><span class="p">,</span><span class="n">epoch_acc</span><span class="p">,</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">phase</span> <span class="o">==</span> <span class="s2">&quot;valid&quot;</span><span class="p">:</span>
                <span class="n">tb_writer</span><span class="o">.</span><span class="n">add_scalar</span><span class="p">(</span><span class="s1">&#39;valid/loss&#39;</span><span class="p">,</span><span class="n">epoch_loss</span><span class="p">,</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
                <span class="n">tb_writer</span><span class="o">.</span><span class="n">add_scalar</span><span class="p">(</span><span class="s1">&#39;valid/acc&#39;</span><span class="p">,</span><span class="n">epoch_acc</span><span class="p">,</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>

        <span class="nb">print</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">epoch</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">save_weight</span><span class="si">}</span><span class="s2">/model_</span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="s2">.pth&quot;</span><span class="p">)</span>

    <span class="n">time_elapsed</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">since</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Training complete in </span><span class="si">{:.0f}</span><span class="s1">m </span><span class="si">{:.0f}</span><span class="s1">s&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
        <span class="n">time_elapsed</span> <span class="o">//</span> <span class="mi">60</span><span class="p">,</span> <span class="n">time_elapsed</span> <span class="o">%</span> <span class="mi">60</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Best val Acc: </span><span class="si">{:4f}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">best_acc</span><span class="p">))</span>

    <span class="c1"># load best model weights</span>
    <span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">best_model_wts</span><span class="p">)</span>
    <span class="n">tb_writer</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">model</span>

<span class="n">model_ft</span> <span class="o">=</span> <span class="n">train_model</span><span class="p">(</span><span class="n">model_ft</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">optimizer_ft</span><span class="p">,</span> <span class="n">exp_lr_scheduler</span><span class="p">,</span><span class="n">num_epochs</span><span class="o">=</span><span class="mi">101</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model_ft</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span><span class="s2">&quot;best_model.pth&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Done!!!&quot;</span><span class="p">)</span>
</code></pre></div>
<p><strong>test.py</strong></p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">transforms</span>
<span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>
<span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">models</span>
<span class="kn">from</span> <span class="nn">torch.optim</span> <span class="kn">import</span> <span class="n">lr_scheduler</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">optim</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">from</span> <span class="nn">PIL</span> <span class="kn">import</span> <span class="n">Image</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">glob</span><span class="o">,</span><span class="nn">os</span><span class="o">,</span><span class="nn">shutil</span>
<span class="kn">import</span> <span class="nn">tqdm</span>

<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda:0&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">load_model</span><span class="p">(</span><span class="n">model_path</span><span class="p">,</span><span class="n">device</span><span class="p">):</span>
    <span class="n">model_ft</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">resnet18</span><span class="p">()</span>
    <span class="n">model_ft</span><span class="o">.</span><span class="n">fc</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">model_ft</span><span class="o">.</span><span class="n">fc</span><span class="o">.</span><span class="n">in_features</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">model_ft</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">model_path</span><span class="p">))</span>
    <span class="n">model_ft</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">model_ft</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">model_ft</span>
<span class="n">model_ft</span> <span class="o">=</span> <span class="n">load_model</span><span class="p">(</span><span class="s2">&quot;best_model.pth&quot;</span><span class="p">,</span><span class="n">device</span><span class="p">)</span>

<span class="n">image_transforms</span> <span class="o">=</span>  <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span>
                    <span class="n">transforms</span><span class="o">.</span><span class="n">Resize</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="mi">256</span><span class="p">),</span>
                    <span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
                    <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">([</span><span class="mf">0.5</span><span class="p">,</span><span class="mf">0.5</span><span class="p">,</span><span class="mf">0.5</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span><span class="mf">0.5</span><span class="p">,</span><span class="mf">0.5</span><span class="p">])</span>
                <span class="p">])</span>

<span class="n">img_path</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>
<span class="n">frame</span> <span class="o">=</span> <span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">img_path</span><span class="p">)</span>
<span class="n">frame</span> <span class="o">=</span> <span class="n">image_transforms</span><span class="p">(</span><span class="n">frame</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">frame</span> <span class="o">=</span> <span class="n">frame</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">model_ft</span><span class="p">(</span><span class="n">frame</span><span class="p">)</span>
<span class="c1"># print(output.data)</span>
<span class="n">preds</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="mi">1</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">preds</span><span class="p">)</span>
</code></pre></div>
<p><strong>训练图片可视化</strong></p>
<div class="highlight"><pre><span></span><code><span class="c1"># 训练照片的可视化，看看经过数据增强后的照片到底是啥样的</span>
<span class="k">def</span> <span class="nf">img_show</span><span class="p">(</span><span class="n">img</span><span class="p">,</span><span class="n">img_nomalize</span><span class="p">,</span><span class="n">title</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">figsize</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">img</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">transpose</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">0</span><span class="p">))</span> <span class="c1"># c,ｈ,w -&gt; h,w,c</span>
    <span class="n">mean</span> <span class="o">=</span> <span class="n">normalize</span><span class="o">.</span><span class="n">mean</span>
    <span class="n">std</span> <span class="o">=</span> <span class="n">normalize</span><span class="o">.</span><span class="n">std</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">std</span><span class="o">*</span><span class="n">img</span><span class="o">+</span><span class="n">mean</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">img</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># 截取</span>
    <span class="k">if</span> <span class="n">figsize</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="n">figsize</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">title</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">pause</span><span class="p">(</span><span class="mf">0.001</span><span class="p">)</span> <span class="c1"># 稍停一下，保证能更新</span>

<span class="c1"># classes:tensor([1, 1, 1, 0, 1, 0, 0, 0])</span>
<span class="n">inputs</span><span class="p">,</span><span class="n">classes</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">dataloaders</span><span class="p">[</span><span class="s1">&#39;train&#39;</span><span class="p">]))</span><span class="c1"># input.shape:torch.Size([8, 3, 224, 224])</span>

<span class="c1"># 将若干幅图像拼成一个图像</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">make_grid</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span><span class="c1"># out.shape-&gt;torch.Size([3, 228, 1810])</span>
<span class="n">img_show</span><span class="p">(</span><span class="n">out</span><span class="p">,</span><span class="n">normalize</span><span class="p">,</span><span class="n">title</span><span class="o">=</span><span class="p">[</span><span class="n">class_names</span><span class="p">[</span><span class="n">x</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">classes</span><span class="p">],</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="mi">14</span><span class="p">))</span>
</code></pre></div>
<p><strong>网格展示生成的图片</strong></p>
<ul>
<li>一个是make_grid，它能将多张图片拼接成一个网格中；</li>
<li>另一个是save_img，它能将Tensor保存成图片。</li>
</ul>
<div class="highlight"><pre><span></span><code><span class="n">torchvision</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">make_grid</span><span class="p">(</span>
    <span class="n">tensor</span><span class="p">,</span> 
    <span class="n">nrow</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> 
    <span class="n">padding</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> 
    <span class="n">normalize</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="c1"># normalize=True, 对图像像素归一化</span>
    <span class="nb">range</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="c1"># range=(min, max)，min和max是数字，则min, max用来规范化image                                                                                   </span>
    <span class="n">scale_each</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span><span class="c1"># scale_each=True, 每个图片独立规范化。</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">torchvision.utils</span> <span class="kn">import</span> <span class="n">make_grid</span><span class="p">,</span> <span class="n">save_image</span>
<span class="n">dataiter</span> <span class="o">=</span> <span class="nb">iter</span><span class="p">(</span><span class="n">dataloader</span><span class="p">)</span>
<span class="n">img</span> <span class="o">=</span> <span class="n">make_grid</span><span class="p">(</span><span class="nb">next</span><span class="p">(</span><span class="n">dataiter</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">4</span><span class="p">)</span> <span class="c1"># 拼成4*4网格图片，且会转成３通道</span>
<span class="n">to_img</span><span class="p">(</span><span class="n">img</span><span class="p">)</span> <span class="c1"># 显示图片</span>

<span class="n">save_image</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="s1">&#39;a.png&#39;</span><span class="p">)</span>
<span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="s1">&#39;a.png&#39;</span><span class="p">)</span>
</code></pre></div>
<p><strong>计算acc+precision+recall</strong></p>
<div class="highlight"><pre><span></span><code><span class="c1"># data[&#39;label&#39;] and data[&#39;prediction&#39;] are groundtruth label and prediction </span>
<span class="c1"># for each image, respectively.</span>
<span class="n">accuracy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;label&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;prediction&#39;</span><span class="p">])</span> <span class="o">*</span> <span class="mi">100</span>

<span class="c1"># Compute recision and recall for each class.</span>
<span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">num_classes</span><span class="p">)):</span>
   <span class="n">tp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">((</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;label&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="n">c</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">),</span>
               <span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;prediction&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="n">c</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">))</span>
   <span class="n">tp_fp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;prediction&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="n">c</span><span class="p">)</span>
   <span class="n">tp_fn</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;label&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="n">c</span><span class="p">)</span>
   <span class="n">precision</span> <span class="o">=</span> <span class="n">tp</span> <span class="o">/</span> <span class="n">tp_fp</span> <span class="o">*</span> <span class="mi">100</span>
   <span class="n">recall</span> <span class="o">=</span> <span class="n">tp</span> <span class="o">/</span> <span class="n">tp_fn</span> <span class="o">*</span> <span class="mi">100</span>
</code></pre></div>
<p><strong>后续高级用法</strong></p>
<p><strong>冻结所有层</strong></p>
<p><a href="http://bbs.cvmart.net/topics/1532?from=timeline">hook函数详解</a></p>
<div class="highlight"><pre><span></span><code><span class="c1"># 将所有的参数层进行冻结,这样带来一个问题:每次这些固定参数的输出是个定量，如果每次都计算，那么很浪费时间</span>
<span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model_ft</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
    <span class="n">param</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">False</span>
<span class="c1"># 如何到处固定层的向量呢？使用PyTorch比较高级的API，hook来处理了，我们要先定义一个hook函数</span>
<span class="n">in_list</span><span class="o">=</span> <span class="p">[]</span> <span class="c1"># 这里存放所有的输出</span>
<span class="k">def</span> <span class="nf">hook</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">output</span><span class="p">):</span>
    <span class="c1">#input是一个tuple代表顺序代表每一个输入项，我们这里只有一项，所以直接获取</span>
    <span class="c1">#需要全部的参数信息可以使用这个打印</span>
    <span class="c1">#for val in input:</span>
    <span class="c1">#    print(&quot;input val:&quot;,val)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">input</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)):</span>
        <span class="n">in_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">input</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
<span class="c1"># 在相应的层注册hook函数，保证函数能够正常工作，我们这里直接hook 全连接层前面的pool层，获取pool层的输入数据，这样会获得更多的特征</span>
<span class="n">model_ft</span><span class="o">.</span><span class="n">avgpool</span><span class="o">.</span><span class="n">register_forward_hook</span><span class="p">(</span><span class="n">hook</span><span class="p">)</span> 
<span class="c1">#上步输出结果 &lt;torch.utils.hooks.RemovableHandle at 0x24812a5e978&gt;对象</span>

<span class="c1"># 开始获取输出，这里我们因为不需要反向传播，所以直接可以使用no_grad嵌套</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">data</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">image_dataloader</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">]):</span>
        <span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="o">=</span> <span class="n">data</span>
        <span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">DEVICE</span><span class="p">)</span>
        <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">DEVICE</span><span class="p">)</span>
        <span class="n">y_hat</span> <span class="o">=</span> <span class="n">model_ft</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="n">features</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">in_list</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s2">&quot;features&quot;</span><span class="p">,</span><span class="n">features</span><span class="p">)</span>

<span class="c1"># 这样再训练时我们只需将这个数组读出来，然后可以直接使用这个数组再输入到linear或者我们前面讲到的sigmod层就可以了。我们在这里在pool层前获取了更多的特征，可以将这些特征使用更高级的分类器，例如SVM，树型的分类器进行分类</span>
</code></pre></div>
<p><strong>冻结部分层</strong></p>
<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">Char3SeqModel</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
   <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">char_sz</span><span class="p">,</span> <span class="n">n_fac</span><span class="p">,</span> <span class="n">n_h</span><span class="p">):</span>
       <span class="nb">super</span><span class="p">(</span><span class="n">Char3SeqModel</span><span class="p">,</span><span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
       <span class="bp">self</span><span class="o">.</span><span class="n">em</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">char_sz</span><span class="p">,</span> <span class="n">n_fac</span><span class="p">)</span>
       <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_fac</span><span class="p">,</span> <span class="n">n_h</span><span class="p">)</span>
       <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_h</span><span class="p">,</span> <span class="n">n_h</span><span class="p">)</span>
       <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_h</span><span class="p">,</span> <span class="n">char_sz</span><span class="p">)</span>

   <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ch1</span><span class="p">,</span> <span class="n">ch2</span><span class="p">,</span> <span class="n">ch3</span><span class="p">):</span>
       <span class="c1"># do something</span>
       <span class="n">out</span> <span class="o">=</span> <span class="c1">#....</span>
       <span class="k">return</span> <span class="n">out</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Char3SeqModel</span><span class="p">(</span><span class="mi">10000</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">25</span><span class="p">)</span>

<span class="c1"># 方法一:冻结fc1层</span>
<span class="n">model</span><span class="o">.</span><span class="n">fc1</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">False</span> <span class="c1"># 冻结</span>
<span class="n">model</span><span class="o">.</span><span class="n">fc1</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">True</span> <span class="c1"># 解冻</span>
<span class="n">optimizer</span><span class="o">.</span><span class="n">add_param_group</span><span class="p">({</span><span class="s1">&#39;params&#39;</span><span class="p">:</span> <span class="n">model</span><span class="o">.</span><span class="n">fc1</span><span class="o">.</span><span class="n">parameters</span><span class="p">()})</span> <span class="c1"># 把fc1层的参数加入，用以反向传播</span>
<span class="c1"># 方法二:冻结fc1层</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">([{</span><span class="s1">&#39;params&#39;</span><span class="p">:[</span> <span class="n">param</span> <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">()</span> <span class="k">if</span> <span class="s1">&#39;fc1&#39;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">name</span><span class="p">]}],</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">optimizer</span><span class="o">.</span><span class="n">add_param_group</span><span class="p">({</span><span class="s1">&#39;params&#39;</span><span class="p">:</span> <span class="n">model</span><span class="o">.</span><span class="n">fc1</span><span class="o">.</span><span class="n">parameters</span><span class="p">()})</span> <span class="c1"># 解冻</span>


<span class="c1"># 最好的实现方式:https://www.zhihu.com/question/311095447/answer/589307812</span>
<span class="kn">from</span> <span class="nn">collections.abc</span> <span class="kn">import</span> <span class="n">Iterable</span>

<span class="k">def</span> <span class="nf">set_freeze_by_names</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">layer_names</span><span class="p">,</span> <span class="n">freeze</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
 <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">layer_names</span><span class="p">,</span> <span class="n">Iterable</span><span class="p">):</span>
     <span class="n">layer_names</span> <span class="o">=</span> <span class="p">[</span><span class="n">layer_names</span><span class="p">]</span>
 <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">child</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">named_children</span><span class="p">():</span>
     <span class="k">if</span> <span class="n">name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">layer_names</span><span class="p">:</span>
         <span class="k">continue</span>
     <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">child</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
         <span class="n">param</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="ow">not</span> <span class="n">freeze</span>

<span class="k">def</span> <span class="nf">freeze_by_names</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">layer_names</span><span class="p">):</span>
 <span class="n">set_freeze_by_names</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">layer_names</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">unfreeze_by_names</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">layer_names</span><span class="p">):</span>
 <span class="n">set_freeze_by_names</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">layer_names</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">set_freeze_by_idxs</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">idxs</span><span class="p">,</span> <span class="n">freeze</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
 <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">idxs</span><span class="p">,</span> <span class="n">Iterable</span><span class="p">):</span>
     <span class="n">idxs</span> <span class="o">=</span> <span class="p">[</span><span class="n">idxs</span><span class="p">]</span>
 <span class="n">num_child</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">children</span><span class="p">()))</span>
 <span class="n">idxs</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">idx</span><span class="p">:</span> <span class="n">num_child</span> <span class="o">+</span> <span class="n">idx</span> <span class="k">if</span> <span class="n">idx</span> <span class="o">&lt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="n">idx</span><span class="p">,</span> <span class="n">idxs</span><span class="p">))</span>
 <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">child</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">children</span><span class="p">()):</span>
     <span class="k">if</span> <span class="n">idx</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">idxs</span><span class="p">:</span>
         <span class="k">continue</span>
     <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">child</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
         <span class="n">param</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="ow">not</span> <span class="n">freeze</span>

<span class="k">def</span> <span class="nf">freeze_by_idxs</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">idxs</span><span class="p">):</span>
 <span class="n">set_freeze_by_idxs</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">idxs</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">unfreeze_by_idxs</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">idxs</span><span class="p">):</span>
 <span class="n">set_freeze_by_idxs</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">idxs</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>

<span class="c1"># 冻结第一层</span>
<span class="n">freeze_by_idxs</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="c1"># 冻结第一、二层</span>
<span class="n">freeze_by_idxs</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="c1">#冻结倒数第一层</span>
<span class="n">freeze_by_idxs</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="c1"># 解冻第一层</span>
<span class="n">unfreeze_by_idxs</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="c1"># 解冻倒数第一层</span>
<span class="n">unfreeze_by_idxs</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="c1"># 冻结 em层</span>
<span class="n">freeze_by_names</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="s1">&#39;em&#39;</span><span class="p">)</span>
<span class="c1"># 冻结 fc1, fc3层</span>
<span class="n">freeze_by_names</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="p">(</span><span class="s1">&#39;fc1&#39;</span><span class="p">,</span> <span class="s1">&#39;fc3&#39;</span><span class="p">))</span>
<span class="c1"># 解冻em, fc1, fc3层</span>
<span class="n">unfreeze_by_names</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="p">(</span><span class="s1">&#39;em&#39;</span><span class="p">,</span> <span class="s1">&#39;fc1&#39;</span><span class="p">,</span> <span class="s1">&#39;fc3&#39;</span><span class="p">))</span>
</code></pre></div>
<h3 id="pytorch16"><strong>Pytorch1.6支持混合精度训练</strong><a class="headerlink" href="#pytorch16" title="Permanent link">&para;</a></h3>
<p>基本思想：精度减半(fp32&rarr;fp16)，训练时间减半</p>
<ul>
<li><code>from torch.cuda.amp import autocast, GradScaler</code></li>
<li><code>GradScaler</code> 对象是 <code>PyTorch</code> 实现的损失缩放，在训练期间，为了防止梯度变小到0(<strong>因为float16无法表示小幅值的变化，反向传播中梯度太小，可能为0</strong>)，某种形式的缩放是必要的。 最佳的损失乘数得足够高以保留非常小的梯度，同时不能太高以至于导致非常大的梯度四舍五入到 <code>inf</code>产生相反的问题。每个网络如何寻找最佳损失乘数？<code>Gradscalar</code> 以一个小的损失乘数开始，这个乘数每次会翻倍。 这种逐渐加倍的行为一直持续到 <code>GradScalar</code> 遇到包含 inf 值的梯度更新。 <code>Gradscalar</code> 丢弃这批数据(例如跳过梯度更新) ，将损失乘数减半，并重置其倍增时间。</li>
<li><code>Gradscaler</code> 需要对梯度更新计算(检查是否溢出)和优化器(将丢弃的<code>batches</code>转换为 <code>no-op</code>)进行控制，以实现其操作(<strong>由于GradScaler()对gradient进行了scale，因此每个参数的gradient应该在optimizer更新参数前unscaled，从而使学习率不受影响。</strong>)。 这就是为什么 <code>loss.backwards()</code>被<code>scaler.scale(loss).backwards()</code>取代， 以及 <code>optimizer.step()</code>被 <code>scaler.step(optimizer)</code>替换的原因。</li>
</ul>
<div class="highlight"><pre><span></span><code><span class="c1"># 单GPU使用</span>
<span class="kn">from</span> <span class="nn">torch.cuda.amp</span> <span class="kn">import</span> <span class="n">autocast</span><span class="p">,</span><span class="n">GradScaler</span>

<span class="c1"># Creates model and optimizer in default precision</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="o">...</span><span class="p">)</span>

<span class="c1"># Creates a GradScaler once at the beginning of training.</span>
<span class="n">use_amp</span> <span class="o">=</span> <span class="kc">True</span> <span class="c1"># 是否使用混合精度训练</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="n">GradScaler</span><span class="p">(</span><span class="n">enabeled</span><span class="o">=</span><span class="n">use_amp</span><span class="p">)</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="n">epochs</span><span class="p">:</span>
    <span class="k">for</span> <span class="nb">input</span><span class="p">,</span> <span class="n">target</span> <span class="ow">in</span> <span class="n">data</span><span class="p">:</span>
        <span class="c1"># model前向+loss开启autocast,实现了 fp32-&gt; fp16转换</span>
        <span class="k">with</span> <span class="n">autocast</span><span class="p">(</span><span class="n">enabled</span><span class="o">=</span><span class="n">use_amp</span><span class="p">):</span>
            <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="c1"># 半精度的数值范围有限，需要用它来放大loss</span>
        <span class="n">scaler</span><span class="o">.</span><span class="n">scale</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span> <span class="c1"># 替换loss.backwards()</span>
        <span class="c1"># 梯度放大后会出现inf或NaN值，如果出现就跳过本次更新，如果不是则用optimizer.step()</span>
        <span class="n">scaler</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)</span> <span class="c1"># 替换optimizer.step()</span>
        <span class="c1"># Updates the scale for next iteration.</span>
        <span class="n">scaler</span><span class="o">.</span><span class="n">update</span><span class="p">()</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="c1"># 多GPU使用</span>
<span class="c1"># 1.下面的方式是不起作用的</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">MyModel</span><span class="p">()</span>
<span class="n">dp_model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">DataParallel</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

<span class="c1"># Sets autocast in the main thread</span>
<span class="k">with</span> <span class="n">autocast</span><span class="p">():</span>
    <span class="c1"># dp_model&#39;s internal threads won&#39;t autocast.  The main thread&#39;s autocast state has no effect.</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">dp_model</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
    <span class="c1"># loss_fn still autocasts, but it&#39;s too late...</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>

<span class="c1"># 2.解决方案，两种</span>
<span class="n">MyModel</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="o">...</span>
    <span class="nd">@autocast</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
       <span class="o">...</span>

<span class="c1"># Alternatively</span>
<span class="n">MyModel</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="o">...</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
        <span class="k">with</span> <span class="n">autocast</span><span class="p">():</span>
            <span class="o">...</span>

<span class="c1"># 3.修改后就可以使用了</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">MyModel</span><span class="p">()</span>
<span class="n">dp_model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">DataParallel</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

<span class="k">with</span> <span class="n">autocast</span><span class="p">():</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">dp_model</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</code></pre></div>
<h3 id="pytorch_7">PyTorch多线程训练<a class="headerlink" href="#pytorch_7" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">torch.multiprocessing</span> <span class="k">as</span> <span class="nn">mp</span>

<span class="kn">from</span> <span class="nn">model</span> <span class="kn">import</span> <span class="n">MyModel</span>

<span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
    <span class="c1"># Construct data_loader, optimizer, etc.</span>
    <span class="k">for</span> <span class="n">data</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">data_loader</span><span class="p">:</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">loss_fn</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">data</span><span class="p">),</span> <span class="n">labels</span><span class="p">)</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>  <span class="c1"># This will update the shared parameters</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">multiprocessing</span><span class="o">.</span><span class="n">set_start_method</span><span class="p">(</span><span class="s1">&#39;spawn&#39;</span><span class="p">)</span>
    <span class="n">num_processes</span> <span class="o">=</span> <span class="mi">4</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">MyModel</span><span class="p">()</span>
    <span class="c1"># NOTE: this is required for the ``fork`` method to work</span>
    <span class="n">model</span><span class="o">.</span><span class="n">share_memory</span><span class="p">()</span>
    <span class="n">processes</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">rank</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_processes</span><span class="p">):</span>
        <span class="n">p</span> <span class="o">=</span> <span class="n">mp</span><span class="o">.</span><span class="n">Process</span><span class="p">(</span><span class="n">target</span><span class="o">=</span><span class="n">train</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">model</span><span class="p">,))</span>
        <span class="n">p</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>
        <span class="n">processes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">processes</span><span class="p">:</span>
        <span class="n">p</span><span class="o">.</span><span class="n">join</span><span class="p">()</span>
</code></pre></div>
<h3 id="_3">问题<a class="headerlink" href="#_3" title="Permanent link">&para;</a></h3>
<p><strong>低版本加载高版本模型报错</strong></p>
<div class="highlight"><pre><span></span><code><span class="c1"># v1.7,用_use_new_zipfile_serialization=False解决</span>
<span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model_</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="s1">&#39;model_best_bacc.pth.tar&#39;</span><span class="p">,</span> <span class="n">_use_new_zipfile_serialization</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="c1">#v1.4</span>
<span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;model_best_bacc.pth.tar&#39;</span><span class="p">,</span><span class="n">map_location</span><span class="o">=</span><span class="s1">&#39;cpu&#39;</span><span class="p">)</span>
</code></pre></div>
                
              
              
                


              
            </article>
          </div>
        </div>
      </main>
      
        
<footer class="md-footer">
  
    <nav class="md-footer__inner md-grid" aria-label="Footer">
      
        <a href="../%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/" class="md-footer__link md-footer__link--prev" rel="prev">
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
          </div>
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                上一页
              </span>
              深度学习基础
            </div>
          </div>
        </a>
      
      
        <a href="../PaddlePaddle%E5%BF%AB%E9%80%9F%E6%95%99%E7%A8%8B/" class="md-footer__link md-footer__link--next" rel="next">
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                下一页
              </span>
              PaddlePaddle快速教程
            </div>
          </div>
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4z"/></svg>
          </div>
        </a>
      
    </nav>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
        Made with
        <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
          Material for MkDocs
        </a>
      </div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    <script id="__config" type="application/json">{"base": "..", "features": [], "translations": {"clipboard.copy": "\u590d\u5236", "clipboard.copied": "\u5df2\u590d\u5236", "search.config.lang": "ja", "search.config.pipeline": "trimmer, stemmer", "search.config.separator": "[\\uff0c\\u3002]+", "search.placeholder": "\u641c\u7d22", "search.result.placeholder": "\u952e\u5165\u4ee5\u5f00\u59cb\u641c\u7d22", "search.result.none": "\u6ca1\u6709\u627e\u5230\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.one": "\u627e\u5230 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.other": "# \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.term.missing": "Missing"}, "search": "../assets/javascripts/workers/search.fb4a9340.min.js", "version": null}</script>
    
    
      <script src="../assets/javascripts/bundle.ca5457b8.min.js"></script>
      
    
  </body>
</html>